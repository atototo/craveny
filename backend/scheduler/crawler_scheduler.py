"""
í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬

APSchedulerë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ê¸°ì ìœ¼ë¡œ ë‰´ìŠ¤ ë° ì£¼ê°€ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""
import logging
from typing import Optional

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger

from backend.crawlers.naver_crawler import NaverNewsCrawler
from backend.crawlers.hankyung_crawler import HankyungNewsCrawler
from backend.crawlers.maeil_crawler import MaeilNewsCrawler
from backend.crawlers.news_saver import NewsSaver
from backend.crawlers.stock_crawler import get_stock_crawler
from backend.crawlers.news_stock_matcher import run_daily_matching
from backend.utils.market_time import is_market_open
from backend.db.session import SessionLocal


logger = logging.getLogger(__name__)


class CrawlerScheduler:
    """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(
        self, news_interval_minutes: int = 10, stock_interval_minutes: int = 1
    ):
        """
        Args:
            news_interval_minutes: ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ê°„ê²© (ë¶„ ë‹¨ìœ„)
            stock_interval_minutes: ì£¼ê°€ ìˆ˜ì§‘ ì‹¤í–‰ ê°„ê²© (ë¶„ ë‹¨ìœ„)
        """
        self.news_interval_minutes = news_interval_minutes
        self.stock_interval_minutes = stock_interval_minutes
        self.scheduler: Optional[BackgroundScheduler] = None
        self.is_running = False

        # ë‰´ìŠ¤ í¬ë¡¤ë§ í†µê³„
        self.news_total_crawls = 0
        self.news_total_saved = 0
        self.news_total_skipped = 0
        self.news_total_errors = 0

        # ì£¼ê°€ ìˆ˜ì§‘ í†µê³„
        self.stock_total_crawls = 0
        self.stock_total_stocks = 0
        self.stock_total_saved = 0
        self.stock_total_errors = 0

        # ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ í†µê³„
        self.matching_total_runs = 0
        self.matching_total_success = 0
        self.matching_total_fail = 0

    def _crawl_all_sources(self) -> None:
        """
        ëª¨ë“  ì–¸ë¡ ì‚¬ì—ì„œ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (#{self.news_total_crawls + 1})")
        logger.info("=" * 60)

        db = SessionLocal()
        saver = NewsSaver(db)

        saved_total = 0
        skipped_total = 0

        try:
            # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with NaverNewsCrawler() as naver:
                    news_list = naver.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… ë„¤ì´ë²„: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  ë„¤ì´ë²„: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.news_total_errors += 1
                logger.error(f"   âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # 2. í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with HankyungNewsCrawler() as hankyung:
                    news_list = hankyung.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… í•œêµ­ê²½ì œ: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  í•œêµ­ê²½ì œ: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.news_total_errors += 1
                logger.error(f"   âŒ í•œêµ­ê²½ì œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # 3. ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with MaeilNewsCrawler() as maeil:
                    news_list = maeil.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… ë§¤ì¼ê²½ì œ: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  ë§¤ì¼ê²½ì œ: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.news_total_errors += 1
                logger.error(f"   âŒ ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.news_total_crawls += 1
            self.news_total_saved += saved_total
            self.news_total_skipped += skipped_total

            # ì„±ê³µë¥  ê³„ì‚°
            success_rate = (
                (self.news_total_crawls - self.news_total_errors) / self.news_total_crawls * 100
                if self.news_total_crawls > 0
                else 0
            )

            logger.info("=" * 60)
            logger.info(f"âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {saved_total}ê±´ ì €ì¥, {skipped_total}ê±´ ìŠ¤í‚µ")
            logger.info(
                f"ğŸ“Š ë‰´ìŠ¤ ì „ì²´ í†µê³„: ì‹¤í–‰ {self.news_total_crawls}íšŒ, "
                f"ì €ì¥ {self.news_total_saved}ê±´, "
                f"ìŠ¤í‚µ {self.news_total_skipped}ê±´, "
                f"ì—ëŸ¬ {self.news_total_errors}íšŒ, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            self.news_total_errors += 1
            logger.error(f"âŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

        finally:
            db.close()

    def _collect_stock_prices(self) -> None:
        """
        ì£¼ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ì¥ ì‹œê°„(09:00~15:30)ì—ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        # ì¥ ì‹œê°„ ì²´í¬
        if not is_market_open():
            logger.debug("â¸ï¸  ì£¼ê°€ ìˆ˜ì§‘ ìŠ¤í‚µ: ì¥ ë§ˆê°")
            return

        logger.info("=" * 60)
        logger.info(f"ğŸ“ˆ ì£¼ê°€ ìˆ˜ì§‘ ì‹œì‘ (#{self.stock_total_crawls + 1})")
        logger.info("=" * 60)

        try:
            # ì£¼ê°€ ìˆ˜ì§‘ê¸° ê°€ì ¸ì˜¤ê¸°
            stock_crawler = get_stock_crawler()

            # Priority 1 ì¢…ëª©ë§Œ ìˆ˜ì§‘ (í•µì‹¬ ëŒ€í˜•ì£¼ 10ê°œ)
            results = stock_crawler.collect_all_stocks(priority=1)

            # í†µê³„ ê³„ì‚°
            total_saved = sum(results.values())
            success_count = sum(1 for count in results.values() if count > 0)
            total_stocks = len(results)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stock_total_crawls += 1
            self.stock_total_stocks += total_stocks
            self.stock_total_saved += total_saved

            # ì‹¤íŒ¨í•œ ì¢…ëª© ìˆ˜
            failed_count = total_stocks - success_count
            if failed_count > 0:
                self.stock_total_errors += failed_count

            # ì„±ê³µë¥  ê³„ì‚°
            success_rate = (success_count / total_stocks * 100) if total_stocks > 0 else 0

            logger.info("=" * 60)
            logger.info(
                f"âœ… ì£¼ê°€ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_stocks}ê°œ ì¢…ëª©, "
                f"ì´ {total_saved}ê±´ ì €ì¥"
            )
            logger.info(
                f"ğŸ“Š ì£¼ê°€ ì „ì²´ í†µê³„: ì‹¤í–‰ {self.stock_total_crawls}íšŒ, "
                f"ì²˜ë¦¬ {self.stock_total_stocks}ê°œ ì¢…ëª©, "
                f"ì €ì¥ {self.stock_total_saved}ê±´, "
                f"ì—ëŸ¬ {self.stock_total_errors}íšŒ, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            self.stock_total_errors += 1
            logger.error(f"âŒ ì£¼ê°€ ìˆ˜ì§‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

    def _match_news_with_stocks(self) -> None:
        """
        ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ë§¤ì¼ ì¥ ë§ˆê° í›„(15:40)ì— ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”— ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì‹œì‘ (#{self.matching_total_runs + 1})")
        logger.info("=" * 60)

        db = SessionLocal()

        try:
            # ì¼ì¼ ë§¤ì¹­ ì‹¤í–‰ (ìµœê·¼ 7ì¼ ë‰´ìŠ¤ ëŒ€ìƒ)
            success_count, fail_count = run_daily_matching(db, lookback_days=7)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.matching_total_runs += 1
            self.matching_total_success += success_count
            self.matching_total_fail += fail_count

            # ì„±ê³µë¥  ê³„ì‚°
            total_attempts = self.matching_total_success + self.matching_total_fail
            success_rate = (
                (self.matching_total_success / total_attempts * 100) if total_attempts > 0 else 0
            )

            logger.info("=" * 60)
            logger.info(f"âœ… ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì™„ë£Œ: ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {fail_count}ê±´")
            logger.info(
                f"ğŸ“Š ë§¤ì¹­ ì „ì²´ í†µê³„: ì‹¤í–‰ {self.matching_total_runs}íšŒ, "
                f"ì„±ê³µ {self.matching_total_success}ê±´, "
                f"ì‹¤íŒ¨ {self.matching_total_fail}ê±´, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

        finally:
            db.close()

    def start(self) -> None:
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.is_running:
            logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return

        logger.info(
            f"ğŸš€ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ "
            f"(ë‰´ìŠ¤: {self.news_interval_minutes}ë¶„, ì£¼ê°€: {self.stock_interval_minutes}ë¶„)"
        )

        self.scheduler = BackgroundScheduler()

        # ë‰´ìŠ¤ í¬ë¡¤ë§ ì‘ì—… ë“±ë¡ (10ë¶„ ê°„ê²©)
        news_trigger = IntervalTrigger(minutes=self.news_interval_minutes)
        self.scheduler.add_job(
            func=self._crawl_all_sources,
            trigger=news_trigger,
            id="news_crawler_job",
            name="ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
            replace_existing=True,
        )

        # ì£¼ê°€ ìˆ˜ì§‘ ì‘ì—… ë“±ë¡ (1ë¶„ ê°„ê²©)
        stock_trigger = IntervalTrigger(minutes=self.stock_interval_minutes)
        self.scheduler.add_job(
            func=self._collect_stock_prices,
            trigger=stock_trigger,
            id="stock_collector_job",
            name="ì£¼ê°€ ìˆ˜ì§‘ê¸°",
            replace_existing=True,
        )

        # ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì‘ì—… ë“±ë¡ (ë§¤ì¼ 15:40)
        matching_trigger = CronTrigger(hour=15, minute=40)
        self.scheduler.add_job(
            func=self._match_news_with_stocks,
            trigger=matching_trigger,
            id="news_stock_matching_job",
            name="ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­",
            replace_existing=True,
        )

        self.scheduler.start()
        self.is_running = True

        logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")

        # ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰
        logger.info("ğŸ”„ ì´ˆê¸° ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰...")
        self._crawl_all_sources()

        # ì¥ ì‹œê°„ì´ë©´ ì£¼ê°€ ìˆ˜ì§‘ë„ ì¦‰ì‹œ ì‹¤í–‰
        if is_market_open():
            logger.info("ğŸ”„ ì´ˆê¸° ì£¼ê°€ ìˆ˜ì§‘ ì‹¤í–‰...")
            self._collect_stock_prices()
        else:
            logger.info("â¸ï¸  ì¥ ë§ˆê° ì‹œê°„ - ì£¼ê°€ ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘")

    def shutdown(self) -> None:
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if not self.is_running:
            logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return

        logger.info("ğŸ›‘ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì¤‘...")

        if self.scheduler:
            self.scheduler.shutdown(wait=False)

        self.is_running = False
        logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")

    def get_stats(self) -> dict:
        """
        í¬ë¡¤ë§ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬ (ë‰´ìŠ¤, ì£¼ê°€, ë§¤ì¹­ í†µê³„)
        """
        # ë‰´ìŠ¤ ì„±ê³µë¥ 
        news_success_rate = (
            (self.news_total_crawls - self.news_total_errors) / self.news_total_crawls * 100
            if self.news_total_crawls > 0
            else 0
        )

        # ì£¼ê°€ ì„±ê³µë¥ 
        stock_success_rate = (
            (self.stock_total_crawls - self.stock_total_errors) / self.stock_total_crawls * 100
            if self.stock_total_crawls > 0
            else 0
        )

        # ë§¤ì¹­ ì„±ê³µë¥ 
        total_matching_attempts = self.matching_total_success + self.matching_total_fail
        matching_success_rate = (
            (self.matching_total_success / total_matching_attempts * 100)
            if total_matching_attempts > 0
            else 0
        )

        return {
            "news": {
                "total_crawls": self.news_total_crawls,
                "total_saved": self.news_total_saved,
                "total_skipped": self.news_total_skipped,
                "total_errors": self.news_total_errors,
                "success_rate": round(news_success_rate, 2),
            },
            "stock": {
                "total_crawls": self.stock_total_crawls,
                "total_stocks": self.stock_total_stocks,
                "total_saved": self.stock_total_saved,
                "total_errors": self.stock_total_errors,
                "success_rate": round(stock_success_rate, 2),
            },
            "matching": {
                "total_runs": self.matching_total_runs,
                "total_success": self.matching_total_success,
                "total_fail": self.matching_total_fail,
                "success_rate": round(matching_success_rate, 2),
            },
            "is_running": self.is_running,
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_crawler_scheduler: Optional[CrawlerScheduler] = None


def get_crawler_scheduler(
    news_interval_minutes: int = 10, stock_interval_minutes: int = 1
) -> CrawlerScheduler:
    """
    CrawlerScheduler ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        news_interval_minutes: ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ê°„ê²© (ë¶„)
        stock_interval_minutes: ì£¼ê°€ ìˆ˜ì§‘ ì‹¤í–‰ ê°„ê²© (ë¶„)

    Returns:
        CrawlerScheduler ì¸ìŠ¤í„´ìŠ¤
    """
    global _crawler_scheduler
    if _crawler_scheduler is None:
        _crawler_scheduler = CrawlerScheduler(news_interval_minutes, stock_interval_minutes)
    return _crawler_scheduler
