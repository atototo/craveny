"""
í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬

APSchedulerë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ê¸°ì ìœ¼ë¡œ ë‰´ìŠ¤ ë° ì£¼ê°€ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""
import logging
from typing import Optional

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger

from backend.crawlers.naver_crawler import NaverNewsCrawler
from backend.crawlers.hankyung_crawler import HankyungNewsCrawler
from backend.crawlers.maeil_crawler import MaeilNewsCrawler
from backend.crawlers.naver_search_crawler import NaverNewsSearchCrawler
from backend.crawlers.dart_crawler import DartCrawler
from backend.crawlers.news_saver import NewsSaver
from backend.crawlers.stock_crawler import get_stock_crawler
from backend.crawlers.news_stock_matcher import run_daily_matching
from backend.llm.embedder import run_daily_embedding
from backend.utils.market_time import is_market_open
from backend.db.session import SessionLocal
from backend.db.models.stock import Stock
from backend.notifications.auto_notify import process_new_news_notifications


logger = logging.getLogger(__name__)


class CrawlerScheduler:
    """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(
        self, news_interval_minutes: int = 10, stock_interval_minutes: int = 1
    ):
        """
        Args:
            news_interval_minutes: ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ê°„ê²© (ë¶„ ë‹¨ìœ„)
            stock_interval_minutes: ì£¼ê°€ ìˆ˜ì§‘ ì‹¤í–‰ ê°„ê²© (ë¶„ ë‹¨ìœ„)
        """
        self.news_interval_minutes = news_interval_minutes
        self.stock_interval_minutes = stock_interval_minutes
        self.scheduler: Optional[BackgroundScheduler] = None
        self.is_running = False

        # ë‰´ìŠ¤ í¬ë¡¤ë§ í†µê³„
        self.news_total_crawls = 0
        self.news_total_saved = 0
        self.news_total_skipped = 0
        self.news_total_errors = 0

        # ì£¼ê°€ ìˆ˜ì§‘ í†µê³„
        self.stock_total_crawls = 0
        self.stock_total_stocks = 0
        self.stock_total_saved = 0
        self.stock_total_errors = 0

        # ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ í†µê³„
        self.matching_total_runs = 0
        self.matching_total_success = 0
        self.matching_total_fail = 0

        # ë‰´ìŠ¤ ì„ë² ë”© í†µê³„
        self.embedding_total_runs = 0
        self.embedding_total_success = 0
        self.embedding_total_fail = 0

        # ìë™ ì•Œë¦¼ í†µê³„
        self.notify_total_runs = 0
        self.notify_total_processed = 0
        self.notify_total_success = 0
        self.notify_total_failed = 0

    def _crawl_all_sources(self) -> None:
        """
        ëª¨ë“  ì–¸ë¡ ì‚¬ì—ì„œ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (#{self.news_total_crawls + 1})")
        logger.info("=" * 60)

        db = SessionLocal()
        saver = NewsSaver(db)

        saved_total = 0
        skipped_total = 0

        try:
            # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with NaverNewsCrawler() as naver:
                    news_list = naver.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… ë„¤ì´ë²„: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  ë„¤ì´ë²„: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.news_total_errors += 1
                logger.error(f"   âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # 2. í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with HankyungNewsCrawler() as hankyung:
                    news_list = hankyung.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… í•œêµ­ê²½ì œ: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  í•œêµ­ê²½ì œ: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.news_total_errors += 1
                logger.error(f"   âŒ í•œêµ­ê²½ì œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # 3. ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with MaeilNewsCrawler() as maeil:
                    news_list = maeil.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… ë§¤ì¼ê²½ì œ: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  ë§¤ì¼ê²½ì œ: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.news_total_errors += 1
                logger.error(f"   âŒ ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.news_total_crawls += 1
            self.news_total_saved += saved_total
            self.news_total_skipped += skipped_total

            # ì„±ê³µë¥  ê³„ì‚°
            success_rate = (
                (self.news_total_crawls - self.news_total_errors) / self.news_total_crawls * 100
                if self.news_total_crawls > 0
                else 0
            )

            logger.info("=" * 60)
            logger.info(f"âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {saved_total}ê±´ ì €ì¥, {skipped_total}ê±´ ìŠ¤í‚µ")
            logger.info(
                f"ğŸ“Š ë‰´ìŠ¤ ì „ì²´ í†µê³„: ì‹¤í–‰ {self.news_total_crawls}íšŒ, "
                f"ì €ì¥ {self.news_total_saved}ê±´, "
                f"ìŠ¤í‚µ {self.news_total_skipped}ê±´, "
                f"ì—ëŸ¬ {self.news_total_errors}íšŒ, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            self.news_total_errors += 1
            logger.error(f"âŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

        finally:
            db.close()

    def _crawl_stock_specific_news(self) -> None:
        """
        ì¢…ëª©ë³„ë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ìˆ˜ì§‘ëŸ‰ ì°¨ë“± ì ìš©.
        """
        logger.info("=" * 60)
        logger.info("ğŸ¯ ì¢…ëª©ë³„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘")
        logger.info("=" * 60)

        db = SessionLocal()
        saver = NewsSaver(db)
        search_crawler = NaverNewsSearchCrawler()

        saved_total = 0
        skipped_total = 0

        try:
            # DBì—ì„œ í™œì„±í™”ëœ ì¢…ëª© ê°€ì ¸ì˜¤ê¸°
            stocks = db.query(Stock).filter(Stock.is_active == True).order_by(Stock.priority).all()

            logger.info(f"ğŸ“Š ê²€ìƒ‰ ëŒ€ìƒ ì¢…ëª©: {len(stocks)}ê°œ")

            for stock in stocks:
                try:
                    # ìš°ì„ ìˆœìœ„ë³„ ìˆ˜ì§‘ëŸ‰ ê²°ì •
                    if stock.priority <= 2:
                        limit = 10  # ë†’ì€ ìš°ì„ ìˆœìœ„
                    elif stock.priority == 3:
                        limit = 5   # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
                    else:
                        limit = 3   # ë‚®ì€ ìš°ì„ ìˆœìœ„

                    logger.info(f"ğŸ” {stock.name} ({stock.code}) ê²€ìƒ‰ ì¤‘... (ìµœëŒ€ {limit}ê±´)")

                    # ì¢…ëª©ëª…ìœ¼ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
                    # NAVERëŠ” í•œê¸€ë¡œ ê²€ìƒ‰ (ì˜ë¬¸ "NAVER"ë¡œ ê²€ìƒ‰í•˜ë©´ ì¶œì²˜ "ë„¤ì´ë²„"ê°€ ëª¨ë‘ ê²€ìƒ‰ë¨)
                    search_query = "ë„¤ì´ë²„" if stock.name == "NAVER" else stock.name

                    news_list = search_crawler.search_news(
                        query=search_query,
                        max_pages=1,
                        max_results=limit
                    )

                    if news_list:
                        # ë‰´ìŠ¤ì— ì¢…ëª©ì½”ë“œ ëª…ì‹œì  ì„¤ì •
                        for news in news_list:
                            news.company_name = stock.name
                            # stock_codeëŠ” news_saverì—ì„œ ìë™ ë§¤ì¹­ë˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥

                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped

                        if saved > 0:
                            logger.info(f"   âœ… {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                        else:
                            logger.debug(f"   â­ï¸  ì „ë¶€ ì¤‘ë³µ ({skipped}ê±´)")
                    else:
                        logger.debug(f"   â„¹ï¸  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

                except Exception as e:
                    logger.error(f"   âŒ {stock.name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            logger.info("=" * 60)
            logger.info(f"âœ… ì¢…ëª©ë³„ ê²€ìƒ‰ ì™„ë£Œ: {saved_total}ê±´ ì €ì¥, {skipped_total}ê±´ ìŠ¤í‚µ")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ ì¢…ëª©ë³„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

        finally:
            db.close()

    def _crawl_dart_disclosures(self) -> None:
        """
        DART ê³µì‹œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        Priority 1-2 ì¢…ëª©ë§Œ ëŒ€ìƒ (ì¤‘ìš” ì¢…ëª©ë§Œ)
        """
        logger.info("=" * 60)
        logger.info("ğŸ“‹ DART ê³µì‹œ ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 60)

        db = SessionLocal()
        saver = NewsSaver(db)
        dart_crawler = DartCrawler()

        # DART API í‚¤ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not dart_crawler.api_key:
            logger.warning("âš ï¸  DART API í‚¤ê°€ ì—†ì–´ ê³µì‹œ ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            logger.info("   API í‚¤ ë°œê¸‰: https://opendart.fss.or.kr/")
            db.close()
            return

        saved_total = 0
        skipped_total = 0

        try:
            # Priority 1-2 ì¢…ëª©ë§Œ ê³µì‹œ ìˆ˜ì§‘ (ì¤‘ìš” ì¢…ëª©)
            stocks = db.query(Stock).filter(
                Stock.is_active == True,
                Stock.priority <= 2
            ).all()

            logger.info(f"ğŸ“Š ê³µì‹œ ìˆ˜ì§‘ ëŒ€ìƒ: {len(stocks)}ê°œ (Priority 1-2ë§Œ)")

            for stock in stocks:
                try:
                    logger.info(f"ğŸ“‹ {stock.name} ({stock.code}) ê³µì‹œ ê²€ìƒ‰ ì¤‘...")

                    # ìµœê·¼ 3ì¼ê°„ ê³µì‹œ ê²€ìƒ‰
                    from datetime import datetime, timedelta
                    disclosures = dart_crawler.fetch_disclosures_by_stock_code(
                        stock_code=stock.code,
                        start_date=datetime.now() - timedelta(days=3),
                        end_date=datetime.now(),
                    )

                    if disclosures:
                        # ê³µì‹œì— ì¢…ëª© ì •ë³´ ì„¤ì •
                        for disclosure in disclosures:
                            disclosure.company_name = stock.name

                        saved, skipped = saver.save_news_batch(disclosures)
                        saved_total += saved
                        skipped_total += skipped

                        if saved > 0:
                            logger.info(f"   âœ… {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                        else:
                            logger.debug(f"   â­ï¸  ì „ë¶€ ì¤‘ë³µ ({skipped}ê±´)")
                    else:
                        logger.debug(f"   â„¹ï¸  ê³µì‹œ ì—†ìŒ")

                except Exception as e:
                    logger.error(f"   âŒ {stock.name} ê³µì‹œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

            logger.info("=" * 60)
            logger.info(f"âœ… DART ê³µì‹œ ìˆ˜ì§‘ ì™„ë£Œ: {saved_total}ê±´ ì €ì¥, {skipped_total}ê±´ ìŠ¤í‚µ")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ DART ê³µì‹œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

        finally:
            db.close()

    def _collect_stock_prices(self) -> None:
        """
        ì£¼ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ì¥ ì‹œê°„(09:00~15:30)ì—ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        # ì¥ ì‹œê°„ ì²´í¬
        if not is_market_open():
            logger.debug("â¸ï¸  ì£¼ê°€ ìˆ˜ì§‘ ìŠ¤í‚µ: ì¥ ë§ˆê°")
            return

        logger.info("=" * 60)
        logger.info(f"ğŸ“ˆ ì£¼ê°€ ìˆ˜ì§‘ ì‹œì‘ (#{self.stock_total_crawls + 1})")
        logger.info("=" * 60)

        try:
            # ì£¼ê°€ ìˆ˜ì§‘ê¸° ê°€ì ¸ì˜¤ê¸°
            stock_crawler = get_stock_crawler()

            # Priority 1 ì¢…ëª©ë§Œ ìˆ˜ì§‘ (í•µì‹¬ ëŒ€í˜•ì£¼ 10ê°œ)
            results = stock_crawler.collect_all_stocks(priority=1)

            # í†µê³„ ê³„ì‚°
            total_saved = sum(results.values())
            success_count = sum(1 for count in results.values() if count > 0)
            total_stocks = len(results)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stock_total_crawls += 1
            self.stock_total_stocks += total_stocks
            self.stock_total_saved += total_saved

            # ì‹¤íŒ¨í•œ ì¢…ëª© ìˆ˜
            failed_count = total_stocks - success_count
            if failed_count > 0:
                self.stock_total_errors += failed_count

            # ì„±ê³µë¥  ê³„ì‚°
            success_rate = (success_count / total_stocks * 100) if total_stocks > 0 else 0

            logger.info("=" * 60)
            logger.info(
                f"âœ… ì£¼ê°€ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_stocks}ê°œ ì¢…ëª©, "
                f"ì´ {total_saved}ê±´ ì €ì¥"
            )
            logger.info(
                f"ğŸ“Š ì£¼ê°€ ì „ì²´ í†µê³„: ì‹¤í–‰ {self.stock_total_crawls}íšŒ, "
                f"ì²˜ë¦¬ {self.stock_total_stocks}ê°œ ì¢…ëª©, "
                f"ì €ì¥ {self.stock_total_saved}ê±´, "
                f"ì—ëŸ¬ {self.stock_total_errors}íšŒ, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            self.stock_total_errors += 1
            logger.error(f"âŒ ì£¼ê°€ ìˆ˜ì§‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

    def _match_news_with_stocks(self) -> None:
        """
        ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ë§¤ì¼ ì¥ ë§ˆê° í›„(15:40)ì— ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”— ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì‹œì‘ (#{self.matching_total_runs + 1})")
        logger.info("=" * 60)

        db = SessionLocal()

        try:
            # ì¼ì¼ ë§¤ì¹­ ì‹¤í–‰ (ìµœê·¼ 7ì¼ ë‰´ìŠ¤ ëŒ€ìƒ)
            success_count, fail_count = run_daily_matching(db, lookback_days=7)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.matching_total_runs += 1
            self.matching_total_success += success_count
            self.matching_total_fail += fail_count

            # ì„±ê³µë¥  ê³„ì‚°
            total_attempts = self.matching_total_success + self.matching_total_fail
            success_rate = (
                (self.matching_total_success / total_attempts * 100) if total_attempts > 0 else 0
            )

            logger.info("=" * 60)
            logger.info(f"âœ… ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì™„ë£Œ: ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {fail_count}ê±´")
            logger.info(
                f"ğŸ“Š ë§¤ì¹­ ì „ì²´ í†µê³„: ì‹¤í–‰ {self.matching_total_runs}íšŒ, "
                f"ì„±ê³µ {self.matching_total_success}ê±´, "
                f"ì‹¤íŒ¨ {self.matching_total_fail}ê±´, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

        finally:
            db.close()

    def _embed_news(self) -> None:
        """
        ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ë§¤ì¼ ì¥ ë§ˆê° í›„(16:00)ì— ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”¤ ë‰´ìŠ¤ ì„ë² ë”© ì‹œì‘ (#{self.embedding_total_runs + 1})")
        logger.info("=" * 60)

        try:
            # ì¼ì¼ ì„ë² ë”© ì‹¤í–‰ (ë°°ì¹˜ 100ê±´)
            success_count, fail_count = run_daily_embedding(batch_size=100)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.embedding_total_runs += 1
            self.embedding_total_success += success_count
            self.embedding_total_fail += fail_count

            # ì„±ê³µë¥  ê³„ì‚°
            total_attempts = self.embedding_total_success + self.embedding_total_fail
            success_rate = (
                (self.embedding_total_success / total_attempts * 100)
                if total_attempts > 0
                else 0
            )

            logger.info("=" * 60)
            logger.info(f"âœ… ë‰´ìŠ¤ ì„ë² ë”© ì™„ë£Œ: ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {fail_count}ê±´")
            logger.info(
                f"ğŸ“Š ì„ë² ë”© ì „ì²´ í†µê³„: ì‹¤í–‰ {self.embedding_total_runs}íšŒ, "
                f"ì„±ê³µ {self.embedding_total_success}ê±´, "
                f"ì‹¤íŒ¨ {self.embedding_total_fail}ê±´, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ì„ë² ë”© ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

    def _auto_notify(self) -> None:
        """
        ìµœê·¼ ë‰´ìŠ¤ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  í…”ë ˆê·¸ë¨ ì•Œë¦¼ì„ ì „ì†¡í•©ë‹ˆë‹¤.
        ë‰´ìŠ¤ í¬ë¡¤ë§ ì§í›„ì— ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”” ìë™ ì•Œë¦¼ ì‹œì‘ (#{self.notify_total_runs + 1})")
        logger.info("=" * 60)

        db = SessionLocal()

        try:
            # ìµœê·¼ 15ë¶„ ì´ë‚´ ë‰´ìŠ¤ ì²˜ë¦¬
            stats = process_new_news_notifications(db, lookback_minutes=15)

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.notify_total_runs += 1
            self.notify_total_processed += stats["processed"]
            self.notify_total_success += stats["success"]
            self.notify_total_failed += stats["failed"]

            # ì„±ê³µë¥  ê³„ì‚°
            total_attempts = self.notify_total_success + self.notify_total_failed
            success_rate = (
                (self.notify_total_success / total_attempts * 100)
                if total_attempts > 0
                else 0
            )

            logger.info("=" * 60)
            logger.info(
                f"âœ… ìë™ ì•Œë¦¼ ì™„ë£Œ: ì²˜ë¦¬ {stats['processed']}ê±´, "
                f"ì„±ê³µ {stats['success']}ê±´, ì‹¤íŒ¨ {stats['failed']}ê±´"
            )
            logger.info(
                f"ğŸ“Š ì•Œë¦¼ ì „ì²´ í†µê³„: ì‹¤í–‰ {self.notify_total_runs}íšŒ, "
                f"ì²˜ë¦¬ {self.notify_total_processed}ê±´, "
                f"ì„±ê³µ {self.notify_total_success}ê±´, "
                f"ì‹¤íŒ¨ {self.notify_total_failed}ê±´, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ ìë™ ì•Œë¦¼ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

        finally:
            db.close()

    def start(self) -> None:
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.is_running:
            logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return

        logger.info(
            f"ğŸš€ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ "
            f"(ë‰´ìŠ¤: {self.news_interval_minutes}ë¶„, ì£¼ê°€: {self.stock_interval_minutes}ë¶„)"
        )

        self.scheduler = BackgroundScheduler()

        # ë‰´ìŠ¤ í¬ë¡¤ë§ ì‘ì—… ë“±ë¡ (10ë¶„ ê°„ê²©)
        news_trigger = IntervalTrigger(minutes=self.news_interval_minutes)
        self.scheduler.add_job(
            func=self._crawl_all_sources,
            trigger=news_trigger,
            id="news_crawler_job",
            name="ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
            replace_existing=True,
        )

        # ì¢…ëª©ë³„ ê²€ìƒ‰ ì‘ì—… ë“±ë¡ (10ë¶„ ê°„ê²©)
        stock_news_trigger = IntervalTrigger(minutes=self.news_interval_minutes)
        self.scheduler.add_job(
            func=self._crawl_stock_specific_news,
            trigger=stock_news_trigger,
            id="stock_news_search_job",
            name="ì¢…ëª©ë³„ ë‰´ìŠ¤ ê²€ìƒ‰",
            replace_existing=True,
        )

        # DART ê³µì‹œ í¬ë¡¤ë§ ì‘ì—… ë“±ë¡ (5ë¶„ ê°„ê²©)
        dart_trigger = IntervalTrigger(minutes=5)
        self.scheduler.add_job(
            func=self._crawl_dart_disclosures,
            trigger=dart_trigger,
            id="dart_disclosure_job",
            name="DART ê³µì‹œ í¬ë¡¤ë§",
            replace_existing=True,
        )

        # ì£¼ê°€ ìˆ˜ì§‘ ì‘ì—… ë“±ë¡ (1ë¶„ ê°„ê²©)
        stock_trigger = IntervalTrigger(minutes=self.stock_interval_minutes)
        self.scheduler.add_job(
            func=self._collect_stock_prices,
            trigger=stock_trigger,
            id="stock_collector_job",
            name="ì£¼ê°€ ìˆ˜ì§‘ê¸°",
            replace_existing=True,
        )

        # ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­ ì‘ì—… ë“±ë¡ (ë§¤ì¼ 15:40)
        matching_trigger = CronTrigger(hour=15, minute=40)
        self.scheduler.add_job(
            func=self._match_news_with_stocks,
            trigger=matching_trigger,
            id="news_stock_matching_job",
            name="ë‰´ìŠ¤-ì£¼ê°€ ë§¤ì¹­",
            replace_existing=True,
        )

        # ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—… ë“±ë¡ (ë§¤ì¼ 16:00)
        embedding_trigger = CronTrigger(hour=16, minute=0)
        self.scheduler.add_job(
            func=self._embed_news,
            trigger=embedding_trigger,
            id="news_embedding_job",
            name="ë‰´ìŠ¤ ì„ë² ë”©",
            replace_existing=True,
        )

        # ìë™ ì•Œë¦¼ ì‘ì—… ë“±ë¡ (ë‰´ìŠ¤ í¬ë¡¤ë§ê³¼ ë™ì¼í•œ ì£¼ê¸°)
        notify_trigger = IntervalTrigger(minutes=self.news_interval_minutes)
        self.scheduler.add_job(
            func=self._auto_notify,
            trigger=notify_trigger,
            id="auto_notify_job",
            name="ìë™ ì•Œë¦¼",
            replace_existing=True,
        )

        self.scheduler.start()
        self.is_running = True

        logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")
        logger.info("â° í¬ë¡¤ëŸ¬ë“¤ì´ ìŠ¤ì¼€ì¤„ì— ë”°ë¼ ìë™ ì‹¤í–‰ë©ë‹ˆë‹¤")
        logger.info("   - ìµœì‹  ë‰´ìŠ¤: 10ë¶„ë§ˆë‹¤")
        logger.info("   - ì¢…ëª©ë³„ ê²€ìƒ‰: 10ë¶„ë§ˆë‹¤")
        logger.info("   - DART ê³µì‹œ: 5ë¶„ë§ˆë‹¤")
        logger.info("   - ì£¼ê°€ ìˆ˜ì§‘: 1ë¶„ë§ˆë‹¤ (ì¥ ì‹œê°„)")

        # ì´ˆê¸° ì‹¤í–‰ì€ ì„ íƒì‚¬í•­ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
        # ì²« ìŠ¤ì¼€ì¤„ê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒì´ ì„œë²„ ì‹œì‘ì„ ë¹ ë¥´ê²Œ í•©ë‹ˆë‹¤
        import os
        if os.getenv("RUN_INITIAL_CRAWL", "false").lower() == "true":
            logger.info("ğŸ”„ ì´ˆê¸° í¬ë¡¤ë§ ì‹¤í–‰...")
            self._crawl_all_sources()
            self._crawl_stock_specific_news()
            self._crawl_dart_disclosures()

            if is_market_open():
                self._collect_stock_prices()
        else:
            logger.info("â­ï¸  ì´ˆê¸° í¬ë¡¤ë§ ìŠ¤í‚µ - ì²« ìŠ¤ì¼€ì¤„ê¹Œì§€ ëŒ€ê¸° ì¤‘...")

    def shutdown(self) -> None:
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if not self.is_running:
            logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return

        logger.info("ğŸ›‘ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì¤‘...")

        if self.scheduler:
            self.scheduler.shutdown(wait=False)

        self.is_running = False
        logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")

    def get_stats(self) -> dict:
        """
        í¬ë¡¤ë§ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬ (ë‰´ìŠ¤, ì£¼ê°€, ë§¤ì¹­, ì„ë² ë”© í†µê³„)
        """
        # ë‰´ìŠ¤ ì„±ê³µë¥ 
        news_success_rate = (
            (self.news_total_crawls - self.news_total_errors) / self.news_total_crawls * 100
            if self.news_total_crawls > 0
            else 0
        )

        # ì£¼ê°€ ì„±ê³µë¥ 
        stock_success_rate = (
            (self.stock_total_crawls - self.stock_total_errors) / self.stock_total_crawls * 100
            if self.stock_total_crawls > 0
            else 0
        )

        # ë§¤ì¹­ ì„±ê³µë¥ 
        total_matching_attempts = self.matching_total_success + self.matching_total_fail
        matching_success_rate = (
            (self.matching_total_success / total_matching_attempts * 100)
            if total_matching_attempts > 0
            else 0
        )

        # ì„ë² ë”© ì„±ê³µë¥ 
        total_embedding_attempts = self.embedding_total_success + self.embedding_total_fail
        embedding_success_rate = (
            (self.embedding_total_success / total_embedding_attempts * 100)
            if total_embedding_attempts > 0
            else 0
        )

        return {
            "news": {
                "total_crawls": self.news_total_crawls,
                "total_saved": self.news_total_saved,
                "total_skipped": self.news_total_skipped,
                "total_errors": self.news_total_errors,
                "success_rate": round(news_success_rate, 2),
            },
            "stock": {
                "total_crawls": self.stock_total_crawls,
                "total_stocks": self.stock_total_stocks,
                "total_saved": self.stock_total_saved,
                "total_errors": self.stock_total_errors,
                "success_rate": round(stock_success_rate, 2),
            },
            "matching": {
                "total_runs": self.matching_total_runs,
                "total_success": self.matching_total_success,
                "total_fail": self.matching_total_fail,
                "success_rate": round(matching_success_rate, 2),
            },
            "embedding": {
                "total_runs": self.embedding_total_runs,
                "total_success": self.embedding_total_success,
                "total_fail": self.embedding_total_fail,
                "success_rate": round(embedding_success_rate, 2),
            },
            "is_running": self.is_running,
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_crawler_scheduler: Optional[CrawlerScheduler] = None


def get_crawler_scheduler(
    news_interval_minutes: int = 10, stock_interval_minutes: int = 1
) -> CrawlerScheduler:
    """
    CrawlerScheduler ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        news_interval_minutes: ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ê°„ê²© (ë¶„)
        stock_interval_minutes: ì£¼ê°€ ìˆ˜ì§‘ ì‹¤í–‰ ê°„ê²© (ë¶„)

    Returns:
        CrawlerScheduler ì¸ìŠ¤í„´ìŠ¤
    """
    global _crawler_scheduler
    if _crawler_scheduler is None:
        _crawler_scheduler = CrawlerScheduler(news_interval_minutes, stock_interval_minutes)
    return _crawler_scheduler
