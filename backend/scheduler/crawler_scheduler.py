"""
ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬

APSchedulerë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ê¸°ì ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""
import logging
from typing import Optional

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger

from backend.crawlers.naver_crawler import NaverNewsCrawler
from backend.crawlers.hankyung_crawler import HankyungNewsCrawler
from backend.crawlers.maeil_crawler import MaeilNewsCrawler
from backend.crawlers.news_saver import NewsSaver
from backend.db.session import SessionLocal


logger = logging.getLogger(__name__)


class CrawlerScheduler:
    """í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self, interval_minutes: int = 10):
        """
        Args:
            interval_minutes: í¬ë¡¤ë§ ì‹¤í–‰ ê°„ê²© (ë¶„ ë‹¨ìœ„)
        """
        self.interval_minutes = interval_minutes
        self.scheduler: Optional[BackgroundScheduler] = None
        self.is_running = False

        # í¬ë¡¤ë§ í†µê³„
        self.total_crawls = 0
        self.total_saved = 0
        self.total_skipped = 0
        self.total_errors = 0

    def _crawl_all_sources(self) -> None:
        """
        ëª¨ë“  ì–¸ë¡ ì‚¬ì—ì„œ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (#{self.total_crawls + 1})")
        logger.info("=" * 60)

        db = SessionLocal()
        saver = NewsSaver(db)

        saved_total = 0
        skipped_total = 0

        try:
            # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with NaverNewsCrawler() as naver:
                    news_list = naver.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… ë„¤ì´ë²„: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  ë„¤ì´ë²„: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.total_errors += 1
                logger.error(f"   âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # 2. í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with HankyungNewsCrawler() as hankyung:
                    news_list = hankyung.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… í•œêµ­ê²½ì œ: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  í•œêµ­ê²½ì œ: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.total_errors += 1
                logger.error(f"   âŒ í•œêµ­ê²½ì œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # 3. ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§
            try:
                logger.info("ğŸ“° ë§¤ì¼ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§...")
                with MaeilNewsCrawler() as maeil:
                    news_list = maeil.fetch_news(limit=10)
                    if news_list:
                        saved, skipped = saver.save_news_batch(news_list)
                        saved_total += saved
                        skipped_total += skipped
                        logger.info(f"   âœ… ë§¤ì¼ê²½ì œ: {saved}ê±´ ì €ì¥, {skipped}ê±´ ìŠ¤í‚µ")
                    else:
                        logger.warning("   âš ï¸  ë§¤ì¼ê²½ì œ: ë‰´ìŠ¤ ì—†ìŒ")
            except Exception as e:
                self.total_errors += 1
                logger.error(f"   âŒ ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.total_crawls += 1
            self.total_saved += saved_total
            self.total_skipped += skipped_total

            # ì„±ê³µë¥  ê³„ì‚°
            success_rate = (
                (self.total_crawls - self.total_errors) / self.total_crawls * 100
                if self.total_crawls > 0
                else 0
            )

            logger.info("=" * 60)
            logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {saved_total}ê±´ ì €ì¥, {skipped_total}ê±´ ìŠ¤í‚µ")
            logger.info(
                f"ğŸ“Š ì „ì²´ í†µê³„: ì‹¤í–‰ {self.total_crawls}íšŒ, "
                f"ì €ì¥ {self.total_saved}ê±´, "
                f"ìŠ¤í‚µ {self.total_skipped}ê±´, "
                f"ì—ëŸ¬ {self.total_errors}íšŒ, "
                f"ì„±ê³µë¥  {success_rate:.1f}%"
            )
            logger.info("=" * 60)

        except Exception as e:
            self.total_errors += 1
            logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")

        finally:
            db.close()

    def start(self) -> None:
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
        if self.is_running:
            logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            return

        logger.info(f"ğŸš€ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ê°„ê²©: {self.interval_minutes}ë¶„)")

        self.scheduler = BackgroundScheduler()

        # IntervalTrigger ì„¤ì • (10ë¶„ ê°„ê²©)
        trigger = IntervalTrigger(minutes=self.interval_minutes)

        # í¬ë¡¤ë§ ì‘ì—… ë“±ë¡
        self.scheduler.add_job(
            func=self._crawl_all_sources,
            trigger=trigger,
            id="news_crawler_job",
            name="ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
            replace_existing=True,
        )

        self.scheduler.start()
        self.is_running = True

        logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")

        # ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰
        logger.info("ğŸ”„ ì´ˆê¸° í¬ë¡¤ë§ ì‹¤í–‰...")
        self._crawl_all_sources()

    def shutdown(self) -> None:
        """ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if not self.is_running:
            logger.warning("ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return

        logger.info("ğŸ›‘ í¬ë¡¤ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì¤‘...")

        if self.scheduler:
            self.scheduler.shutdown(wait=False)

        self.is_running = False
        logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ ì™„ë£Œ")

    def get_stats(self) -> dict:
        """
        í¬ë¡¤ë§ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        success_rate = (
            (self.total_crawls - self.total_errors) / self.total_crawls * 100
            if self.total_crawls > 0
            else 0
        )

        return {
            "total_crawls": self.total_crawls,
            "total_saved": self.total_saved,
            "total_skipped": self.total_skipped,
            "total_errors": self.total_errors,
            "success_rate": round(success_rate, 2),
            "is_running": self.is_running,
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_crawler_scheduler: Optional[CrawlerScheduler] = None


def get_crawler_scheduler(interval_minutes: int = 10) -> CrawlerScheduler:
    """
    CrawlerScheduler ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        interval_minutes: í¬ë¡¤ë§ ì‹¤í–‰ ê°„ê²©

    Returns:
        CrawlerScheduler ì¸ìŠ¤í„´ìŠ¤
    """
    global _crawler_scheduler
    if _crawler_scheduler is None:
        _crawler_scheduler = CrawlerScheduler(interval_minutes)
    return _crawler_scheduler
