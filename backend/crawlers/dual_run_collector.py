"""
Dual-Run ëª¨ë“œ ì¼ë´‰ ìˆ˜ì§‘ê¸°

FDRê³¼ KIS ë°ì´í„°ë¥¼ ë™ì‹œì— ìˆ˜ì§‘í•˜ì—¬ ìë™ ê²€ì¦í•©ë‹ˆë‹¤.
í™˜ê²½ ë³€ìˆ˜ DUAL_RUN_MODE=trueë¡œ í™œì„±í™”.
"""
import logging
import asyncio
import os
from typing import Dict, Optional, List
from datetime import datetime, timedelta

import pandas as pd
from sqlalchemy.orm import Session

from backend.crawlers.stock_crawler import StockCrawler
from backend.crawlers.kis_daily_crawler import KISDailyCrawler
from backend.validators.kis_validator import get_validator
from backend.db.models.stock import Stock
from backend.db.session import SessionLocal


logger = logging.getLogger(__name__)


class DualRunCollector:
    """
    FDR + KIS ë™ì‹œ ìˆ˜ì§‘ ë° ìë™ ê²€ì¦

    1. FDR ë°ì´í„° ìˆ˜ì§‘ (source=fdr)
    2. KIS ë°ì´í„° ìˆ˜ì§‘ (source=kis)
    3. ìë™ ê²€ì¦ ì‹¤í–‰
    4. ê²€ì¦ ê²°ê³¼ ë¡œê¹…
    """

    def __init__(self):
        self.fdr_crawler = StockCrawler(use_db=True)
        self.kis_crawler = KISDailyCrawler()
        self.validator = get_validator()

    async def collect_stock_dual(
        self,
        stock_code: str,
        days: int = 1,
        db: Optional[Session] = None
    ) -> Dict:
        """
        ë‹¨ì¼ ì¢…ëª© Dual-Run ìˆ˜ì§‘

        Args:
            stock_code: ì¢…ëª© ì½”ë“œ
            days: ìˆ˜ì§‘ ê¸°ê°„ (ì¼)
            db: DB ì„¸ì…˜

        Returns:
            ìˆ˜ì§‘ ë° ê²€ì¦ ê²°ê³¼
        """
        start_time = datetime.now()

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š Dual-Run: {stock_code}")
        logger.info(f"{'='*60}")

        db_session = db or SessionLocal()
        should_close = db is None

        try:
            # 1. FDR ìˆ˜ì§‘
            logger.info(f"1ï¸âƒ£  FDR ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            fdr_start = datetime.now() - timedelta(days=days)
            fdr_df = self.fdr_crawler.fetch_stock_data(stock_code, start_date=fdr_start)

            if fdr_df is not None and not fdr_df.empty:
                fdr_count = self.fdr_crawler.save_stock_data(stock_code, fdr_df, db_session)
                logger.info(f"   âœ… FDR ì €ì¥: {fdr_count}ê±´")
            else:
                logger.warning(f"   âš ï¸  FDR ë°ì´í„° ì—†ìŒ")
                fdr_count = 0

            # 2. KIS ìˆ˜ì§‘
            logger.info(f"2ï¸âƒ£  KIS ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            kis_result = await self.kis_crawler.collect_stock(
                stock_code,
                days=days,
                db=db_session
            )
            kis_count = kis_result.get("count", 0)
            logger.info(f"   âœ… KIS ì €ì¥: {kis_count}ê±´")

            # 3. ìë™ ê²€ì¦
            logger.info(f"3ï¸âƒ£  ìë™ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
            validation_results = self._validate_collected_data(
                stock_code,
                days=days,
                db=db_session
            )

            # 4. ê²€ì¦ ê²°ê³¼ ìš”ì•½
            if validation_results:
                metrics = self.validator.calculate_metrics(validation_results)
                logger.info(f"   ğŸ“Š ê²€ì¦ ê²°ê³¼:")
                logger.info(f"      - ë¹„êµ ê±´ìˆ˜: {metrics['total_count']}ê±´")
                logger.info(f"      - ì¼ì¹˜ìœ¨: {metrics['match_rate']:.2f}%")
                logger.info(f"      - í‰ê·  ì°¨ì´: {metrics['avg_diff_close_pct']:.3f}%")
                logger.info(f"      - ì´ìƒì¹˜: {metrics['anomaly_count']}ê±´")

                # ì´ìƒì¹˜ ìƒì„¸
                anomalies = [r for r in validation_results if r.is_anomaly]
                if anomalies:
                    logger.warning(f"   âš ï¸  ì´ìƒì¹˜ ë°œê²¬:")
                    for a in anomalies[:3]:  # ìµœëŒ€ 3ê±´ë§Œ ì¶œë ¥
                        logger.warning(
                            f"      {a.date}: FDR={a.fdr_close:,.0f}, "
                            f"KIS={a.kis_close:,.0f} (ì°¨ì´ {a.diff_close_pct:.2f}%)"
                        )
            else:
                logger.warning(f"   âš ï¸  ê²€ì¦í•  ë°ì´í„° ì—†ìŒ")
                metrics = {}

            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"{'='*60}")
            logger.info(f"âœ… Dual-Run ì™„ë£Œ ({elapsed:.1f}s)\n")

            return {
                "stock_code": stock_code,
                "status": "success",
                "fdr_count": fdr_count,
                "kis_count": kis_count,
                "validation": metrics,
                "elapsed_time": elapsed
            }

        except Exception as e:
            logger.error(f"âŒ Dual-Run ì‹¤íŒ¨: {stock_code}, {e}")
            return {
                "stock_code": stock_code,
                "status": "failed",
                "error": str(e)
            }

        finally:
            if should_close:
                db_session.close()

    def _validate_collected_data(
        self,
        stock_code: str,
        days: int,
        db: Session
    ) -> List:
        """
        ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦

        Args:
            stock_code: ì¢…ëª© ì½”ë“œ
            days: ê²€ì¦ ê¸°ê°„
            db: DB ì„¸ì…˜

        Returns:
            ê²€ì¦ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days + 1)  # ì—¬ìœ ë¶„ ì¶”ê°€

        results = self.validator.validate_stock(
            stock_code,
            start_date=start_date,
            end_date=end_date
        )

        return results

    async def collect_all_dual(
        self,
        days: int = 1,
        batch_size: int = 10
    ) -> Dict:
        """
        ì „ì²´ ì¢…ëª© Dual-Run ìˆ˜ì§‘

        Args:
            days: ìˆ˜ì§‘ ê¸°ê°„
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ Dual-Run ëª¨ë“œ ì‹œì‘ (ì „ì²´ ì¢…ëª©, {days}ì¼)")
        logger.info(f"{'='*80}\n")

        db = SessionLocal()

        try:
            # í™œì„± ì¢…ëª© ì¡°íšŒ
            stocks = db.query(Stock).filter(Stock.is_active == True).all()
            stock_codes = [stock.code for stock in stocks]

            logger.info(f"ìˆ˜ì§‘ ëŒ€ìƒ: {len(stock_codes)}ê°œ ì¢…ëª©\n")

            results = []
            success_count = 0
            total_fdr = 0
            total_kis = 0

            # ë°°ì¹˜ ì²˜ë¦¬
            for i in range(0, len(stock_codes), batch_size):
                batch = stock_codes[i:i + batch_size]

                logger.info(f"\nğŸ“¦ ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ì¢…ëª©")

                # ë°°ì¹˜ ë‚´ ë³‘ë ¬ ìˆ˜ì§‘
                tasks = [
                    self.collect_stock_dual(code, days=days, db=db)
                    for code in batch
                ]

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                for result in batch_results:
                    if isinstance(result, Exception):
                        logger.error(f"ì˜ˆì™¸ ë°œìƒ: {result}")
                        continue

                    results.append(result)

                    if result["status"] == "success":
                        success_count += 1
                        total_fdr += result.get("fdr_count", 0)
                        total_kis += result.get("kis_count", 0)

                # Rate limiting
                if i + batch_size < len(stock_codes):
                    await asyncio.sleep(0.5)

            # ì „ì²´ ê²°ê³¼ ìš”ì•½
            success_rate = (success_count / len(stock_codes)) * 100 if stock_codes else 0

            # ì „ì²´ ê²€ì¦ í†µê³„
            all_validations = []
            for r in results:
                if r["status"] == "success" and r.get("validation"):
                    all_validations.append(r["validation"])

            if all_validations:
                avg_match_rate = sum(v["match_rate"] for v in all_validations) / len(all_validations)
                avg_diff_pct = sum(v["avg_diff_close_pct"] for v in all_validations) / len(all_validations)
                total_anomalies = sum(v["anomaly_count"] for v in all_validations)
            else:
                avg_match_rate = 0
                avg_diff_pct = 0
                total_anomalies = 0

            summary = {
                "total_stocks": len(stock_codes),
                "success_count": success_count,
                "failed_count": len(stock_codes) - success_count,
                "success_rate": success_rate,
                "total_fdr_saved": total_fdr,
                "total_kis_saved": total_kis,
                "avg_match_rate": avg_match_rate,
                "avg_diff_pct": avg_diff_pct,
                "total_anomalies": total_anomalies,
                "results": results
            }

            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ‰ Dual-Run ì „ì²´ ì™„ë£Œ!")
            logger.info(f"{'='*80}")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
            logger.info(f"   - ì „ì²´: {len(stock_codes)}ê°œ")
            logger.info(f"   - ì„±ê³µ: {success_count}ê°œ ({success_rate:.1f}%)")
            logger.info(f"   - ì‹¤íŒ¨: {len(stock_codes) - success_count}ê°œ")
            logger.info(f"   - FDR ì €ì¥: {total_fdr}ê±´")
            logger.info(f"   - KIS ì €ì¥: {total_kis}ê±´")
            logger.info(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
            logger.info(f"   - í‰ê·  ì¼ì¹˜ìœ¨: {avg_match_rate:.2f}%")
            logger.info(f"   - í‰ê·  ì°¨ì´: {avg_diff_pct:.3f}%")
            logger.info(f"   - ì´ ì´ìƒì¹˜: {total_anomalies}ê±´")
            logger.info(f"{'='*80}\n")

            return summary

        finally:
            db.close()


# ì‹±ê¸€í†¤
_collector: Optional[DualRunCollector] = None


def get_dual_run_collector() -> DualRunCollector:
    """Dual-Run Collector ì‹±ê¸€í†¤"""
    global _collector
    if _collector is None:
        _collector = DualRunCollector()
    return _collector


def is_dual_run_enabled() -> bool:
    """
    í™˜ê²½ ë³€ìˆ˜ë¡œ Dual-Run ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ í™•ì¸

    Returns:
        True: Dual-Run ëª¨ë“œ í™œì„±í™”
        False: ì¼ë°˜ ëª¨ë“œ
    """
    return os.getenv("DUAL_RUN_MODE", "false").lower() == "true"
