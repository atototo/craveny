"""
Reddit í¬ë¡¤ëŸ¬ êµ¬í˜„

PRAW (Python Reddit API Wrapper)ë¥¼ ì‚¬ìš©í•˜ì—¬ Reddit ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import logging
from typing import List, Optional
from datetime import datetime, timedelta

import praw
from praw.models import Submission

from backend.crawlers.base_crawler import BaseNewsCrawler, NewsArticleData
from backend.config import settings


logger = logging.getLogger(__name__)


class RedditCrawler(BaseNewsCrawler):
    """Reddit í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        subreddits: Optional[List[str]] = None,
        keywords: Optional[List[str]] = None,
        min_upvotes: int = 10,
        min_comments: int = 2,
        lookback_hours: int = 24,
    ):
        """
        Args:
            subreddits: ìˆ˜ì§‘í•  subreddit ëª©ë¡ (ê¸°ë³¸ê°’: config ì„¤ì •)
            keywords: í•„í„°ë§ í‚¤ì›Œë“œ ëª©ë¡ (ê¸°ë³¸ê°’: config ì„¤ì •)
            min_upvotes: ìµœì†Œ upvote ìˆ˜ (ê¸°ë³¸ê°’: 10)
            min_comments: ìµœì†Œ ëŒ“ê¸€ ìˆ˜ (ê¸°ë³¸ê°’: 2)
            lookback_hours: ìˆ˜ì§‘ ê¸°ê°„ (ì‹œê°„, ê¸°ë³¸ê°’: 24)
        """
        super().__init__(
            source_name="reddit",
            timeout=30,
            max_retries=3,
            rate_limit_seconds=2.0,  # Reddit API rate limit
        )

        # Reddit API ì´ˆê¸°í™”
        self.reddit = praw.Reddit(
            client_id=settings.REDDIT_CLIENT_ID,
            client_secret=settings.REDDIT_CLIENT_SECRET,
            user_agent=settings.REDDIT_USER_AGENT,
        )

        # í¬ë¡¤ë§ ì„¤ì •
        self.subreddits = subreddits or settings.REDDIT_SUBREDDITS.split(',')
        self.keywords = keywords or settings.REDDIT_KEYWORDS.split(',')
        self.min_upvotes = min_upvotes
        self.min_comments = min_comments
        self.lookback_hours = lookback_hours

        # í‚¤ì›Œë“œ ì •ê·œí™” (ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì œê±°)
        self.keywords = [kw.strip().lower() for kw in self.keywords]

        logger.info(
            f"RedditCrawler ì´ˆê¸°í™”: "
            f"subreddits={self.subreddits}, "
            f"keywords={len(self.keywords)}ê°œ, "
            f"lookback_hours={self.lookback_hours}"
        )

    def _is_relevant(self, submission: Submission) -> bool:
        """
        ê²Œì‹œê¸€ì´ ìˆ˜ì§‘ ëŒ€ìƒì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.

        Args:
            submission: Reddit ê²Œì‹œê¸€

        Returns:
            ìˆ˜ì§‘ ëŒ€ìƒ ì—¬ë¶€
        """
        # 1. upvote ìˆ˜ í™•ì¸
        if submission.score < self.min_upvotes:
            return False

        # 2. ëŒ“ê¸€ ìˆ˜ í™•ì¸
        if submission.num_comments < self.min_comments:
            return False

        # 3. ì‘ì„± ì‹œê°„ í™•ì¸
        created_time = datetime.fromtimestamp(submission.created_utc)
        cutoff_time = datetime.utcnow() - timedelta(hours=self.lookback_hours)
        if created_time < cutoff_time:
            return False

        # 4. í‚¤ì›Œë“œ í™•ì¸ (ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨)
        text = f"{submission.title} {submission.selftext}".lower()

        for keyword in self.keywords:
            if keyword in text:
                return True

        return False

    def _submission_to_news_data(self, submission: Submission) -> NewsArticleData:
        """
        Reddit ê²Œì‹œê¸€ì„ NewsArticleDataë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            submission: Reddit ê²Œì‹œê¸€

        Returns:
            NewsArticleData ê°ì²´
        """
        # ì œëª©
        title = submission.title

        # ë³¸ë¬¸ (selftextê°€ ì—†ìœ¼ë©´ ì œëª©ë§Œ ì‚¬ìš©)
        content = submission.selftext if submission.selftext else submission.title

        # ë°œí‘œ ì‹œê°„
        published_at = datetime.fromtimestamp(submission.created_utc)

        # ì†ŒìŠ¤ ì‹ë³„ì (ì˜ˆ: "reddit:r/stocks")
        source = f"reddit:r/{submission.subreddit.display_name}"

        # URL
        url = f"https://www.reddit.com{submission.permalink}"

        # ì‘ì„±ì
        author = str(submission.author) if submission.author else "[deleted]"

        # ë©”íƒ€ë°ì´í„°
        metadata = {
            "upvotes": submission.score,
            "num_comments": submission.num_comments,
            "subreddit": submission.subreddit.display_name,
            "post_id": submission.id,
            "upvote_ratio": submission.upvote_ratio,
            "is_self": submission.is_self,
            "link_flair_text": submission.link_flair_text,
        }

        return NewsArticleData(
            title=title,
            content=content,
            published_at=published_at,
            source=source,
            url=url,
            author=author,
            metadata=metadata,
        )

    def fetch_news(self, limit: int = 100) -> List[NewsArticleData]:
        """
        Reddit ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        Args:
            limit: subredditë‹¹ ê°€ì ¸ì˜¬ ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜

        Returns:
            NewsArticleData ë¦¬ìŠ¤íŠ¸
        """
        all_news = []

        for subreddit_name in self.subreddits:
            try:
                logger.info(f"r/{subreddit_name} í¬ë¡¤ë§ ì‹œì‘...")

                subreddit = self.reddit.subreddit(subreddit_name)

                # ìµœê·¼ ê²Œì‹œê¸€ ìˆ˜ì§‘ (hot, new ì¡°í•©)
                submissions = list(subreddit.hot(limit=limit // 2)) + \
                             list(subreddit.new(limit=limit // 2))

                relevant_count = 0

                for submission in submissions:
                    # Rate limiting ì ìš©
                    self._apply_rate_limit()

                    # ê´€ë ¨ì„± ê²€ì‚¬
                    if not self._is_relevant(submission):
                        continue

                    # NewsArticleDataë¡œ ë³€í™˜
                    try:
                        news_data = self._submission_to_news_data(submission)
                        all_news.append(news_data)
                        relevant_count += 1

                        logger.debug(
                            f"âœ… {submission.title[:50]} "
                            f"(â†‘{submission.score}, ğŸ’¬{submission.num_comments})"
                        )

                    except Exception as e:
                        logger.warning(f"ê²Œì‹œê¸€ ë³€í™˜ ì‹¤íŒ¨: {submission.id}, {e}")
                        continue

                logger.info(
                    f"r/{subreddit_name} í¬ë¡¤ë§ ì™„ë£Œ: "
                    f"{relevant_count}ê°œ ìˆ˜ì§‘ (ì´ {len(submissions)}ê°œ ì¤‘)"
                )

            except Exception as e:
                logger.error(f"r/{subreddit_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}", exc_info=True)
                continue

        logger.info(f"Reddit í¬ë¡¤ë§ ì´ {len(all_news)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
        return all_news


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    crawler = RedditCrawler()
    news_list = crawler.fetch_news(limit=50)

    print(f"\nì´ {len(news_list)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘:")
    for news in news_list[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
        print(f"\nì œëª©: {news.title}")
        print(f"ì†ŒìŠ¤: {news.source}")
        print(f"ì‘ì„±ì: {news.author}")
        print(f"URL: {news.url}")
        print(f"Upvotes: {news.metadata.get('upvotes')}")
        print(f"Comments: {news.metadata.get('num_comments')}")
