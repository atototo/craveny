"""
ë‰´ìŠ¤ ì €ì¥ ë¡œì§

í¬ë¡¤ë§í•œ ë‰´ìŠ¤ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
"""
import logging
from typing import Optional, List
from datetime import datetime

from sqlalchemy.orm import Session

from backend.crawlers.base_crawler import NewsArticleData
from backend.db.models.news import NewsArticle, ContentType
from backend.db.models.prediction import Prediction
from backend.utils.stock_mapping import get_stock_mapper
from backend.utils.deduplicator import get_deduplicator
from backend.utils.embedding_deduplicator import get_embedding_deduplicator
from backend.utils.encoding_normalizer import get_encoding_normalizer
from backend.llm.predictor import StockPredictor
from backend.llm.vector_search import get_vector_search
from backend.services.stock_analysis_service import update_stock_analysis_summary
import asyncio


logger = logging.getLogger(__name__)


class NewsSaver:
    """ë‰´ìŠ¤ ì €ì¥ í´ë˜ìŠ¤"""

    def __init__(self, db: Session, auto_predict: bool = True):
        """
        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            auto_predict: ë‰´ìŠ¤ ì €ì¥ ì‹œ ìë™ ì˜ˆì¸¡ ì‹¤í–‰ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        self.db = db
        self.auto_predict = auto_predict
        self.stock_mapper = get_stock_mapper()
        self.deduplicator = get_deduplicator()
        self.embedding_deduplicator = get_embedding_deduplicator()
        self.encoding_normalizer = get_encoding_normalizer()

        # ìë™ ì˜ˆì¸¡ì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ predictor ì´ˆê¸°í™”
        self.predictor = None
        if self.auto_predict:
            try:
                self.predictor = StockPredictor()
                logger.info("ìë™ ì˜ˆì¸¡ ì‹œìŠ¤í…œ í™œì„±í™”")
            except Exception as e:
                logger.warning(f"ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨, ìë™ ì˜ˆì¸¡ ë¹„í™œì„±í™”: {e}")
                self.auto_predict = False

    def _determine_content_type(self, source: str) -> str:
        """
        ì†ŒìŠ¤ ì‹ë³„ìì—ì„œ ì½˜í…ì¸  íƒ€ì…ì„ ê²°ì •í•©ë‹ˆë‹¤.

        Args:
            source: ì†ŒìŠ¤ ì‹ë³„ì (ì˜ˆ: "naver", "reddit:r/stocks")

        Returns:
            content_type ë¬¸ìì—´ (ì†Œë¬¸ì)
        """
        if source.startswith('reddit:'):
            return 'reddit'
        elif source.startswith('twitter:'):
            return 'twitter'
        elif source.startswith('telegram:'):
            return 'telegram'
        else:
            return 'news'

    def _extract_stock_code(self, news_data: NewsArticleData) -> Optional[str]:
        """
        ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ ì¢…ëª©ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„°

        Returns:
            ì¢…ëª©ì½”ë“œ ë˜ëŠ” None
        """
        # 1. ë‰´ìŠ¤ ë°ì´í„°ì— ê¸°ì—…ëª…ì´ ìˆìœ¼ë©´ ì§ì ‘ ë§¤í•‘
        if news_data.company_name:
            stock_code = self.stock_mapper.get_stock_code(news_data.company_name)
            if stock_code:
                logger.debug(f"ê¸°ì—…ëª…ìœ¼ë¡œ ì¢…ëª©ì½”ë“œ ë§¤ì¹­: {news_data.company_name} -> {stock_code}")
                return stock_code

        # 2. ì œëª©ì—ì„œ ê¸°ì—…ëª… ì°¾ê¸°
        stock_code = self.stock_mapper.find_stock_code_in_text(news_data.title)
        if stock_code:
            logger.debug(f"ì œëª©ì—ì„œ ì¢…ëª©ì½”ë“œ ë°œê²¬: {news_data.title[:50]} -> {stock_code}")
            return stock_code

        # 3. ë³¸ë¬¸ì—ì„œ ê¸°ì—…ëª… ì°¾ê¸°
        stock_code = self.stock_mapper.find_stock_code_in_text(news_data.content)
        if stock_code:
            logger.debug(f"ë³¸ë¬¸ì—ì„œ ì¢…ëª©ì½”ë“œ ë°œê²¬ -> {stock_code}")
            return stock_code

        logger.debug("ì¢…ëª©ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None

    def save_news(self, news_data: NewsArticleData) -> Optional[NewsArticle]:
        """
        ë‰´ìŠ¤ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

        ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìˆ˜í–‰í•˜ê³ , ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            news_data: ë‰´ìŠ¤ ë°ì´í„°

        Returns:
            ì €ì¥ëœ NewsArticle ë˜ëŠ” None (ì¤‘ë³µì¸ ê²½ìš°)
        """
        # ì¸ì½”ë”© ê²€ì¦ ë° ì •ê·œí™”
        title = news_data.title
        content = news_data.content

        if self.encoding_normalizer.has_broken_text(title):
            logger.warning(f"ê¹¨ì§„ ì œëª© ê°ì§€: {title[:50]}, ë³µêµ¬ ì‹œë„")
            title = self.encoding_normalizer.try_fix_broken_encoding(title)
            if not title:
                logger.error(f"ì œëª© ë³µêµ¬ ì‹¤íŒ¨, ë‰´ìŠ¤ ìŠ¤í‚µ")
                return None

        if self.encoding_normalizer.has_broken_text(content):
            logger.warning(f"ê¹¨ì§„ ë³¸ë¬¸ ê°ì§€, ë³µêµ¬ ì‹œë„")
            content = self.encoding_normalizer.try_fix_broken_encoding(content)

        # ì¤‘ë³µ ê²€ì‚¬
        is_duplicate, duplicate_id = self.deduplicator.find_duplicate_in_db(
            title, self.db
        )

        if is_duplicate:
            logger.info(f"ì¤‘ë³µ ë‰´ìŠ¤ ìŠ¤í‚µ: {title[:50]}")
            return None

        # ì¢…ëª©ì½”ë“œ ì¶”ì¶œ
        stock_code = self._extract_stock_code(news_data)

        # content_type ê²°ì •
        content_type = self._determine_content_type(news_data.source)

        # Reddit/Twitter ì „ìš© í•„ë“œ ì¶”ì¶œ
        upvotes = news_data.metadata.get('upvotes')
        num_comments = news_data.metadata.get('num_comments')
        subreddit = news_data.metadata.get('subreddit')

        # NewsArticle ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        news_article = NewsArticle(
            title=title,
            content=content,
            published_at=news_data.published_at,
            source=news_data.source,
            stock_code=stock_code,
            # Multi-platform í•„ë“œ
            content_type=content_type,
            url=news_data.url,
            author=news_data.author,
            upvotes=upvotes,
            num_comments=num_comments,
            subreddit=subreddit,
            extra_metadata=news_data.metadata,
        )

        # DBì— ì €ì¥
        try:
            self.db.add(news_article)
            self.db.commit()
            self.db.refresh(news_article)

            logger.info(
                f"ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ: ID={news_article.id}, "
                f"ì œëª©='{news_article.title[:50]}', "
                f"ì¢…ëª©ì½”ë“œ={stock_code or 'N/A'}"
            )

            # ìë™ ì˜ˆì¸¡ ì‹¤í–‰ (ì¢…ëª©ì½”ë“œê°€ ìˆì„ ë•Œë§Œ)
            if self.auto_predict and self.predictor and stock_code:
                self._run_prediction(news_article, stock_code)

            return news_article

        except Exception as e:
            self.db.rollback()
            logger.error(f"ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def _run_prediction(self, news_article: NewsArticle, stock_code: str):
        """
        ë‰´ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì‹¤í–‰í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            news_article: ì €ì¥ëœ ë‰´ìŠ¤ ê¸°ì‚¬
            stock_code: ì¢…ëª© ì½”ë“œ
        """
        try:
            logger.info(f"ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘: ë‰´ìŠ¤ ID={news_article.id}, ì¢…ëª©={stock_code}")

            # 0. ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬ (ì˜ˆì¸¡ skip ì—¬ë¶€ í™•ì¸)
            news_text = f"{news_article.title} {news_article.content}"
            should_skip, similar_id, similarity = self.embedding_deduplicator.should_skip_prediction(
                news_text=news_text,
                stock_code=stock_code,
                db=self.db,
            )

            if should_skip:
                logger.info(
                    f"ğŸ”´ ì„ë² ë”© ìœ ì‚¬ë„ ë†’ìŒ ({similarity:.3f}) â†’ ì˜ˆì¸¡ skip "
                    f"(ë‰´ìŠ¤ ID={news_article.id}, ìœ ì‚¬ ë‰´ìŠ¤ ID={similar_id})"
                )
                return

            # 1. ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰
            vector_search = get_vector_search()
            similar_news = vector_search.get_news_with_price_changes(
                news_text=news_text,
                stock_code=stock_code,
                db=self.db,
                top_k=5,
                similarity_threshold=0.7
            )

            logger.info(f"ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(similar_news)}ê±´")

            # 2. ì˜ˆì¸¡ ì‹¤í–‰
            current_news = {
                "title": news_article.title,
                "content": news_article.content,
                "stock_code": stock_code,
            }

            prediction_result = self.predictor.predict(
                current_news=current_news,
                similar_news=similar_news,
                news_id=news_article.id,
                use_cache=True
            )

            if prediction_result:
                # ì˜ˆì¸¡ ë°©í–¥ ë³€í™˜ (í•œê¸€ â†’ ì˜ë¬¸)
                prediction_text = prediction_result.get("prediction", "ìœ ì§€")
                direction_map = {"ìƒìŠ¹": "up", "í•˜ë½": "down", "ìœ ì§€": "hold"}
                direction = direction_map.get(prediction_text, "hold")

                # ì‹ ë¢°ë„ ë³€í™˜ (0-100 â†’ 0.0-1.0)
                confidence_percent = prediction_result.get("confidence", 0)
                confidence = confidence_percent / 100.0

                # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DBì— ì €ì¥
                prediction = Prediction(
                    news_id=news_article.id,
                    stock_code=stock_code,
                    direction=direction,
                    confidence=confidence,
                    reasoning=prediction_result.get("reasoning", ""),
                    current_price=prediction_result.get("current_price"),
                    target_period="1ì¼"
                )

                self.db.add(prediction)
                self.db.commit()

                logger.info(
                    f"ì˜ˆì¸¡ ì €ì¥ ì™„ë£Œ: ë‰´ìŠ¤ ID={news_article.id}, "
                    f"ë°©í–¥={prediction.direction}, ì‹ ë¢°ë„={prediction.confidence:.2f}"
                )

                # ìƒˆ ì˜ˆì¸¡ ì €ì¥ í›„ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸
                try:
                    logger.info(f"ì¢…ëª© {stock_code}ì˜ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸ ì‹œì‘")
                    asyncio.run(update_stock_analysis_summary(stock_code, self.db, force_update=False))
                    logger.info(f"ì¢…ëª© {stock_code}ì˜ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                except Exception as report_error:
                    logger.error(f"ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {report_error}", exc_info=True)
                    # ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨í•´ë„ ì˜ˆì¸¡ ì €ì¥ì€ ìœ ì§€

            else:
                logger.warning(f"ì˜ˆì¸¡ ê²°ê³¼ ì—†ìŒ: ë‰´ìŠ¤ ID={news_article.id}")

        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì‹¤í–‰ ì‹¤íŒ¨: ë‰´ìŠ¤ ID={news_article.id}, {e}", exc_info=True)
            # ì˜ˆì¸¡ ì‹¤íŒ¨í•´ë„ ë‰´ìŠ¤ ì €ì¥ì€ ìœ ì§€
            self.db.rollback()

    def save_news_batch(
        self, news_list: List[NewsArticleData]
    ) -> tuple[int, int]:
        """
        ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ ë°°ì¹˜ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            news_list: ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            (ì €ì¥ ì„±ê³µ ìˆ˜, ì¤‘ë³µ ìŠ¤í‚µ ìˆ˜) íŠœí”Œ
        """
        saved_count = 0
        skipped_count = 0

        for news_data in news_list:
            result = self.save_news(news_data)
            if result:
                saved_count += 1
            else:
                skipped_count += 1

        logger.info(
            f"ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: ì´ {len(news_list)}ê±´ -> "
            f"ì €ì¥ {saved_count}ê±´, ì¤‘ë³µ ìŠ¤í‚µ {skipped_count}ê±´"
        )

        return (saved_count, skipped_count)
