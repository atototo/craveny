# Story 1.4: news-crawler

**Epic:** 1 - 데이터 수집 및 저장 인프라
**Status:** Draft

---

## Story

**As a** 시스템,
**I want** 10분마다 한국 주요 언론사의 증권 뉴스를 자동으로 수집하여 PostgreSQL에 저장하고,
**so that** 최신 뉴스 데이터가 지속적으로 축적된다.

---

## Acceptance Criteria

1. 뉴스 크롤러(`backend/crawlers/news_crawler.py`)가 네이버 뉴스, 한국경제, 매일경제를 크롤링한다.
2. APScheduler로 10분마다 크롤러가 자동 실행된다.
3. 각 뉴스에서 제목, 본문, 발표 시간, 종목코드(기업명 매칭)를 추출하여 `news` 테이블에 저장한다.
4. 중복 뉴스는 제목 유사도로 필터링하여 저장하지 않는다.
5. 크롤링 실패 시 에러 로그를 기록하고, 다음 주기에 재시도한다.
6. 크롤링 성공률을 추적하는 로그가 생성된다.
7. 종목코드 매핑 테이블(`stock_codes.json`)이 제공되어 기업명 → 종목코드 변환이 가능하다.

---

## Tasks / Subtasks

- [ ] Task 1: 종목코드 매핑 데이터 준비 (AC: 7)
  - [ ] `data/stock_codes.json` 생성
  - [ ] 주요 대형주 50개 기업명-종목코드 매핑
  - [ ] JSON 로드 유틸리티 함수 (`backend/utils/stock_mapping.py`)

- [ ] Task 2: 뉴스 크롤러 Base 클래스 (AC: 1)
  - [ ] `backend/crawlers/base_crawler.py` 작성
  - [ ] BaseNewsCrawler 추상 클래스
  - [ ] `fetch_news()` 추상 메서드
  - [ ] HTTP 요청 공통 로직 (timeout, retry)

- [ ] Task 3: 네이버 뉴스 크롤러 (AC: 1, 3)
  - [ ] `backend/crawlers/naver_crawler.py` 작성
  - [ ] BeautifulSoup4로 HTML 파싱
  - [ ] 증권 섹션 URL 설정
  - [ ] 제목, 본문, 발표시간, 기업명 추출

- [ ] Task 4: 한국경제, 매일경제 크롤러 (AC: 1, 3)
  - [ ] `backend/crawlers/hankyung_crawler.py`
  - [ ] `backend/crawlers/maeil_crawler.py`
  - [ ] 각 사이트 HTML 구조에 맞게 파싱 로직 작성

- [ ] Task 5: 중복 필터링 로직 (AC: 4)
  - [ ] `backend/utils/deduplicator.py` 작성
  - [ ] 제목 Levenshtein 거리 계산 (80% 유사도)
  - [ ] 최근 24시간 뉴스만 중복 검사 대상

- [ ] Task 6: 뉴스 저장 로직 (AC: 3)
  - [ ] `backend/crawlers/news_saver.py` 작성
  - [ ] 기업명 → 종목코드 매칭
  - [ ] NewsArticle 모델 인스턴스 생성
  - [ ] DB 세션에 추가 및 커밋

- [ ] Task 7: APScheduler 스케줄러 설정 (AC: 2)
  - [ ] `backend/scheduler/crawler_scheduler.py` 작성
  - [ ] BackgroundScheduler 설정
  - [ ] 10분 주기 IntervalTrigger
  - [ ] 크롤링 함수 등록

- [ ] Task 8: 에러 처리 및 로깅 (AC: 5, 6)
  - [ ] 크롤링 실패 시 try-except
  - [ ] 로깅 레벨: INFO (성공), ERROR (실패)
  - [ ] 성공률 메트릭 (Redis에 카운터 저장)

- [ ] Task 9: FastAPI 통합 (AC: 2)
  - [ ] `backend/main.py`에 스케줄러 시작 로직 추가
  - [ ] startup 이벤트에서 scheduler.start()
  - [ ] shutdown 이벤트에서 scheduler.shutdown()

---

## Dev Notes

### News Sources

**네이버 뉴스:**
- URL: `https://finance.naver.com/news/news_list.nhn?mode=LSS2D&section_id=101&section_id2=258`
- 구조: `<div class="articleSubject">` 제목, `<div class="articleSummary">` 본문

**한국경제:**
- URL: `https://www.hankyung.com/finance/stock`
- 구조: 확인 필요

**매일경제:**
- URL: `https://www.mk.co.kr/news/stock/`
- 구조: 확인 필요

### Crawling Best Practices

- User-Agent 설정: "Mozilla/5.0 (Compatible; Craveny/1.0)"
- Timeout: 10초
- Retry: 최대 3회, exponential backoff
- Rate Limiting: 각 사이트당 1초 간격

### Deduplication Algorithm

```python
from difflib import SequenceMatcher

def is_duplicate(title1: str, title2: str, threshold=0.8) -> bool:
    similarity = SequenceMatcher(None, title1, title2).ratio()
    return similarity >= threshold
```

### Stock Code Mapping

```json
{
  "삼성전자": "005930",
  "SK하이닉스": "000660",
  "NAVER": "035420",
  ...
}
```

매칭 로직:
1. 뉴스 본문에서 기업명 키워드 검색
2. 매핑 테이블에서 종목코드 조회
3. 매칭 실패 시 `stock_code=NULL`

### Testing

**Test Location**: `tests/unit/test_crawlers.py`, `tests/integration/test_news_crawler.py`

**Testing Requirements:**
- Unit: 파싱 로직, 중복 필터링, 종목코드 매핑
- Integration: 실제 웹사이트 크롤링 (mocking 고려)
- Fixture: 샘플 HTML 응답

**Test Cases:**
1. 네이버 뉴스 HTML 파싱 성공
2. 중복 제목 필터링 (80% 유사도)
3. 기업명 → 종목코드 매칭
4. 크롤링 실패 시 재시도
5. APScheduler 10분 주기 동작

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-31 | 1.0 | Story created from Epic 1 | Sarah (PO) |

---

## Dev Agent Record

### Agent Model Used

_To be populated by Dev Agent_

### Debug Log References

_To be populated by Dev Agent_

### Completion Notes List

_To be populated by Dev Agent_

### File List

_To be populated by Dev Agent_

---

## QA Results

_To be populated by QA Agent_
