# Craveny Product Requirements Document (PRD)

**Version:** 1.1
**Date:** 2025-10-27
**Status:** Validated - Ready for Architecture

---

## Goals and Background Context

### Goals

- LLM + RAG 접근 방식이 뉴스의 주가 영향도를 80% 이상 사용자 만족도로 예측 가능한지 검증
- 개인 투자자가 뉴스 발표 후 5분 이내에 자신감 있는 타이밍 결정("지금 사도 늦지 않았나?")을 할 수 있도록 지원
- 지속적으로 지식 베이스를 구축하는 안정적인 데이터 수집 파이프라인 구축 (95% 이상 가동률)
- 시간대별 맞춤 전략(프리마켓/장중/장후)이 포함된 실행 가능한 텔레그램 알림 제공
- 20명 이상의 테스트 사용자 확보 및 피드백 수집으로 Phase 2 우선순위 검증
- 핵심 예측 및 알림 기능을 구현한 2주 MVP 배포 달성

### Background Context

한국 개인 투자자(1,400만 명, 전체 인구의 27%)는 중대한 정보 비대칭에 직면해 있습니다. 매일 수백 건의 뉴스를 접하지만, 영향도의 크기, 타이밍의 적절성, 과거 선례를 판단할 능력이 부족합니다. 이로 인해 타이밍 결정이 잘못되고 연평균 수익률이 -5%에서 -8%로, 기관 투자자에 비해 현저히 낮은 성과를 보입니다.

기존 솔루션(네이버 증권, 증권사 리포트)은 뉴스 집계만 제공할 뿐 예측 분석이나 타이밍 가이드를 제공하지 않습니다. 최근 LLM 기술의 발전으로 새로운 접근이 가능해졌습니다: RAG(검색 증강 생성)를 사용하여 과거 뉴스-주가 패턴을 검색하고, 자연어 설명과 함께 맥락적 예측을 생성하는 것입니다. 이 2주 MVP는 LLM 기반 예측을 시간대별 전략 및 텔레그램 전송과 결합하여 개인 투자자에게 충분한 가치를 제공하는지 검증하고, Phase 2(공시, 재무제표, 연관 종목 추천)로의 확장을 정당화할 수 있는지 확인하는 데 초점을 맞춥니다.

### Change Log

| 날짜 | 버전 | 설명 | 작성자 |
|------|------|------|--------|
| 2025-10-26 | 1.0 | 프로젝트 브리프 기반 초기 PRD 생성 | John (PM) |
| 2025-10-27 | 1.1 | PM Checklist 검증 완료 및 개선 사항 반영 (NFR18-21 추가, Technical Risks 추가) | John (PM) |

---

## Requirements

### 기능 요구사항 (Functional Requirements)

**데이터 수집 및 저장:**

1. **FR1**: 시스템은 10분 간격으로 네이버 뉴스, 한국경제, 매일경제 등 주요 언론사의 증권 뉴스를 자동 수집해야 한다.
2. **FR2**: 시스템은 1분 간격으로 한국 증권시장(코스피/코스닥)의 주가 데이터(시가/고가/저가/종가/거래량)를 수집해야 한다.
3. **FR3**: 시스템은 뉴스 발표 후 1일, 3일, 5일 시점의 주가 변동률을 자동으로 계산하여 뉴스와 매칭해야 한다.
4. **FR4**: 시스템은 수집된 뉴스를 임베딩하여 벡터 DB에 저장하고, 메타데이터(종목코드, 주가 변동률, 뉴스 카테고리)와 함께 관리해야 한다.
5. **FR5**: 시스템은 최소 과거 3개월(500~1,000건) 이상의 뉴스-주가 패턴 데이터를 초기 구축해야 한다.

**LLM 기반 예측 엔진:**

6. **FR6**: 새로운 뉴스 발생 시, 시스템은 벡터 DB에서 임베딩 유사도 기반으로 과거 유사 뉴스 TOP 5를 검색해야 한다.
7. **FR7**: 시스템은 현재 뉴스, 과거 유사 뉴스, 현재 주가 정보를 LLM에 제공하여 종합 분석을 수행해야 한다.
8. **FR8**: 시스템은 예측 결과로 상승/하락 확률(%), 영향도 점수(0~10), 예상 변동폭(%), 영향 지속 기간(일)을 출력해야 한다.
9. **FR9**: 시스템은 예측 근거를 자연어로 생성해야 한다 (예: "과거 독점 계약 뉴스 15건 중 12건 상승, 평균 +7.2%").

**시간대별 맞춤 전략:**

10. **FR10**: 시스템은 뉴스 발표 시간을 감지하여 프리마켓(장 시작 전)/장중(거래 시간)/장후(마감 후)를 자동 판별해야 한다.
11. **FR11**: 프리마켓 시나리오의 경우, 시스템은 예상 시가, 동시호가 전략, 초반 조정 패턴을 포함한 메시지를 생성해야 한다.
12. **FR12**: 장중 시나리오의 경우, 시스템은 현재 상승률, 뉴스 반영도(%), 추가 상승 여력, 적정 진입 타이밍을 포함한 메시지를 생성해야 한다.
13. **FR13**: 장후 시나리오의 경우, 시스템은 다음날 예상 시가, 준비 사항, 목표가/손절가를 포함한 메시지를 생성해야 한다.

**텔레그램 알림:**

14. **FR14**: 시스템은 영향도 점수 8.0 이상의 뉴스만 필터링하여 알림 대상으로 선정해야 한다.
15. **FR15**: 시스템은 뉴스 발생 후 5분 이내에 텔레그램으로 알림을 전송해야 한다.
16. **FR16**: 사용자는 회원가입 없이 텔레그램 봇을 추가하는 것만으로 서비스를 시작할 수 있어야 한다.
17. **FR17**: 텔레그램 메시지는 뉴스 제목, 예측 결과(확률, 영향도, 변동폭), 과거 패턴 통계, 시간대별 전략을 포함해야 한다.

**종목 지원:**

18. **FR18**: 시스템은 한국 증권시장의 코스피/코스닥 상장 종목을 지원해야 한다.
19. **FR19**: MVP는 대형주(시가총액 상위)를 우선 지원하며, 중소형주는 데이터 충분성에 따라 단계적으로 확대해야 한다.

---

### 비기능 요구사항 (Non-Functional Requirements)

**성능:**

1. **NFR1**: 뉴스 발생부터 LLM 분석 완료 및 텔레그램 알림 전송까지 5분 이내에 완료되어야 한다.
2. **NFR2**: LLM 응답 시간은 단일 분석 기준 2~5초 이내여야 한다.
3. **NFR3**: 뉴스 크롤러는 10분 주기를 95% 이상 준수해야 한다.
4. **NFR4**: 주가 수집기는 1분 주기를 98% 이상 준수해야 한다.

**안정성:**

5. **NFR5**: 데이터 수집 파이프라인의 2주간 가동률은 95% 이상이어야 한다.
6. **NFR6**: 시스템 전체 다운타임은 2주 동안 1시간 미만이어야 한다.
7. **NFR7**: 뉴스-주가 매칭 정확도는 90% 이상이어야 한다.

**정확도 및 품질:**

8. **NFR8**: LLM 분석 품질에 대한 사용자 평가 "도움됨"이 80% 이상이어야 한다.
9. **NFR9**: 영향도 8.0 이상으로 필터링된 뉴스의 실제 주가 변동률은 평균 ±5% 이상이어야 한다.

**비용:**

10. **NFR10**: MVP 총 비용은 $100 이하로 유지되어야 한다 (OpenAI API $50, 서버 $50).
11. **NFR11**: LLM API 비용은 1건당 $0.02 이하로 유지되어야 한다.

**보안:**

12. **NFR12**: OpenAI API 키, 텔레그램 봇 토큰 등 민감 정보는 환경 변수(.env)로 관리되어야 한다.
13. **NFR13**: 사용자 개인정보는 텔레그램 user_id만 저장하며 최소 수집 원칙을 준수해야 한다.

**확장성:**

14. **NFR14**: 벡터 DB는 매일 50~100건의 새 뉴스를 추가할 수 있도록 설계되어야 한다.
15. **NFR15**: 시스템은 사용자 100명까지 확장 가능하도록 설계되어야 한다 (MVP 목표).

**유지보수성:**

16. **NFR16**: 뉴스 소스 사이트 구조 변경 시, 크롤러 수정 없이 백업 소스로 자동 전환되어야 한다.
17. **NFR17**: LLM 프롬프트는 코드 수정 없이 설정 파일에서 변경 가능해야 한다.

**백업 및 복구:**

18. **NFR18**: PostgreSQL 데이터는 매일 자정(KST 00:00)에 자동 백업되어야 하며, 최소 7일간 백업본이 보관되어야 한다.
19. **NFR19**: Milvus 벡터 데이터는 매주 일요일 자정에 백업되어야 하며, 최소 4주간 백업본이 보관되어야 한다.

**데이터 보관 정책:**

20. **NFR20**: 사용자 데이터(telegram_users)는 마지막 활동일로부터 6개월간 보관 후 익명화 처리되어야 한다.
21. **NFR21**: 뉴스 원본 데이터는 수집일로부터 1년간 보관 후, 요약본(제목, 종목코드, 주가 변동률)만 영구 보관되어야 한다.

---

## User Interface Design Goals

**⚠️ 참고:** MVP는 텔레그램 봇 기반이며, 별도 웹/앱 UI는 없습니다. 이 섹션은 텔레그램 메시지 포맷 가이드라인을 정의합니다.

### 전체 UX 비전

**핵심 원칙: "5초 안에 이해하고, 30초 안에 결정할 수 있는 정보 전달"**

텔레그램 메시지는 투자자가 모바일에서 빠르게 읽고 즉시 행동할 수 있도록 설계됩니다. 정보는 계층적으로 배치되어 가장 중요한 정보(호재/악재, 확률)가 먼저 보이고, 상세 정보(근거, 전략)는 그 다음에 제공됩니다.

**UX 목표:**
- **즉시성**: 푸시 알림으로 뉴스 발생 5분 이내 도달
- **명확성**: 복잡한 분석 결과를 날씨 예보처럼 직관적으로 표현
- **실행 가능성**: "지금 무엇을 해야 하는가?"에 대한 명확한 가이드
- **신뢰성**: 예측 근거(과거 통계)를 투명하게 제시

### 주요 인터랙션 패러다임

**단방향 정보 전달 (MVP)**
- MVP는 봇 → 사용자 단방향 알림만 제공
- 사용자 인터랙션 없음 (구독/구독 해제만 가능)
- Phase 2에서 사용자 피드백 버튼 추가 예정

**정보 계층 구조**
1. **헤드라인** (1-2줄): 종목명 + 뉴스 요약
2. **핵심 예측** (3-4줄): 확률, 영향도, 변동폭
3. **예측 근거** (2-3줄): 과거 패턴 통계
4. **실행 전략** (4-5줄): 시간대별 맞춤 전략

### 핵심 화면 및 뷰

**메시지 타입:**

1. **프리마켓 알림** (장 시작 전 6:00~8:30)
   - 예상 시가 및 급등/급락 예측
   - 동시호가 전략 (참여 vs 관망)
   - 초반 조정 패턴 경고

2. **장중 알림** (거래 시간 9:00~15:20)
   - 현재가 및 뉴스 발표 후 변동률
   - 뉴스 반영도(%) - "지금 사도 늦지 않았나?" 답변
   - 추가 상승/하락 여력 및 적정 진입 타이밍

3. **장후 알림** (마감 후 15:30~)
   - 다음날 예상 시가 및 전략
   - 밤새 체크할 사항 (해외 시장 등)
   - 목표가/손절가 제안

### 접근성

**텔레그램 기본 접근성 준수**
- 텔레그램 앱의 내장 접근성 기능 활용
- 이모지는 보조 수단으로만 사용, 핵심 정보는 텍스트로 제공
- 색상에 의존하지 않는 정보 전달

**Phase 2 웹 대시보드:** WCAG 2.1 AA 준수 목표

### 브랜딩

**MVP: 미니멀 브랜딩**
- 브랜드명 "Craveny" + 이모지 📊 사용
- 전문적이고 신뢰감 있는 톤 유지
- 과도한 이모지 사용 지양

**메시지 톤:**
- 존댓말 사용
- 단정적 표현 지양
- 투자 조언 아님을 명시

### 대상 디바이스 및 플랫폼

**플랫폼:** 텔레그램 (크로스 플랫폼)
- iOS, Android, Windows, macOS, Linux 모두 지원

**디바이스 최적화:**
- **모바일 우선**: 메시지 길이 15줄 이내 (스크롤 최소화)

### 메시지 포맷 상세 규칙

#### 텍스트 포맷팅

**텔레그램 마크다운 사용:**
- `*굵은 텍스트*` - 종목명, 핵심 수치
- `_기울임_` - 부가 설명

**이모지 사용 원칙:**
- 📊 Craveny 봇 시그니처
- 🗞️ 뉴스 제목
- 📈 상승 예측
- 📉 하락 예측
- ⏰ 시간 정보
- ⚠️ 경고/주의사항
- 💡 전략 제안
- 📅 날짜 정보

#### 정보 밀도

**필수 포함 정보:**
1. 종목명 + 현재가
2. 뉴스 제목 (30자 이내)
3. 상승/하락 확률(%)
4. 영향도 점수(0~10)
5. 과거 패턴 통계
6. 시간대별 전략

#### 메시지 길이

- **최대 길이**: 4096자 (텔레그램 제한)
- **권장 길이**: 500~800자
- **최소 길이**: 300자

### 메시지 예시 (프리마켓)

```
📊 *SK하이닉스* 중요 뉴스 알림 (07:15)

🗞️ "SK하이닉스, 엔비디아와 HBM3 독점 공급 계약 체결"
📅 뉴스 발표: 07:00
⏰ 장 시작까지: 1시간 45분

📈 AI 예측:
- 전날 종가: 142,500원
- *예상 시가: 148,000~150,000원 (+3.9~5.3%)*
- 상승 지속 확률: *78%* (3~5일간)
- 영향도 점수: *8.5/10*

⚠️ 프리마켓 전략:
- 추천: 동시호가 관망 후 9:05~9:30 진입
- 이유: 시초가 과열 가능성, 초반 조정 후 재상승 패턴

📊 과거 패턴:
- 유사 뉴스 시초가 평균: +4.2%
- 이후 재상승 확률: 83%

_본 정보는 투자 조언이 아니며, 최종 투자 결정은 본인 책임입니다._
```

---

## Technical Assumptions

### 레포지토리 구조

**선택: Monorepo**

```
craveny/
├── backend/
│   ├── crawlers/       # 뉴스/주가 크롤러
│   ├── llm/            # LLM 프롬프트, RAG 로직
│   ├── telegram/       # 텔레그램 봇
│   ├── db/             # PostgreSQL 모델
│   └── scheduler/      # 스케줄링 로직
├── data/               # 수집된 원본 데이터
├── scripts/            # 초기 데이터 수집 스크립트
├── tests/              # 테스트 코드
└── docs/               # 문서
```

**선택 이유:** MVP는 단일 팀 개발, 코드 공유 용이, CI/CD 단순화

### 서비스 아키텍처

**선택: Monolith (단일 FastAPI 서버)**

**구성:**
- 단일 FastAPI 애플리케이션
- 백그라운드 작업: APScheduler
- 비동기 작업: Celery + Redis

**선택 이유:** 2주 MVP에 최적, 사용자 100명 규모에 충분, Phase 2에서 마이크로서비스 전환 가능

### 테스팅 요구사항

**선택: Unit + Integration 테스트**

**테스트 범위:**
- **Unit**: LLM 프롬프트 생성, 시간대 판별, 뉴스-주가 매칭, 영향도 필터링
- **Integration**: 크롤러 → DB, 임베딩 → 벡터 DB, LLM API, 텔레그램 API
- **E2E**: 수동 테스트

**테스트 프레임워크:** pytest, pytest-asyncio

**커버리지 목표:** Unit 70% 이상

**선택 이유:** MVP에 E2E 자동화는 과도, 핵심 로직은 Unit 테스트로 충분

### 추가 기술 가정

#### 백엔드 기술 스택

**언어 및 프레임워크:**
- Python 3.11+
- FastAPI (비동기 처리)
- APScheduler (크롤링 스케줄)
- Celery + Redis (비동기 작업)

**선택 이유:** Python은 데이터/ML/API에 최적, FastAPI는 비동기 우수, APScheduler는 간단한 스케줄링에 충분

#### LLM 및 임베딩

**LLM:** OpenAI GPT-4o-mini (기본), 필요 시 GPT-4o 업그레이드

**Embedding:** OpenAI text-embedding-3-small (768차원)

**선택 이유:** GPT-4o-mini는 비용 대비 성능 우수 ($0.01~0.02/건), 안정적 API

**대안:** GPT-4o (품질 향상 필요 시), Claude 3.5 Sonnet (백업)

#### 데이터베이스

**PostgreSQL:**
- 뉴스 원본 데이터
- 주가 데이터
- 뉴스-주가 매칭 결과
- 텔레그램 사용자

**벡터 DB: Milvus (Docker)**

**구성:**
```yaml
services:
  milvus:
    image: milvusdb/milvus:latest
    ports:
      - "19530:19530"
    volumes:
      - ./milvus_data:/var/lib/milvus
    depends_on:
      - etcd
      - minio
```

**선택 이유:**
- ✅ 완전 무료, 벡터 수 제한 없음
- ✅ 데이터 주권 확보
- ✅ Phase 2 확장 시 추가 비용 없음
- ✅ Docker로 로컬/프로덕션 동일 관리
- ✅ 프로덕션 레벨 검증됨

**Trade-off:** 초기 설정 +0.5일 (Pinecone 대비), 직접 운영 필요 → 장기적 비용 절감 효과 큼

#### 데이터 수집

**뉴스 크롤링:**
- BeautifulSoup4 (HTML 파싱)
- Scrapy (복잡한 크롤링 시)
- 백업: 뉴스 API

**주가 데이터:**
- FinanceDataReader (한국 주가, 무료)
- yfinance (백업)

**선택 이유:** BeautifulSoup4는 간단하고 충분, FinanceDataReader는 한국 시장 특화

#### 텔레그램

**라이브러리:** python-telegram-bot

**선택 이유:** 텔레그램 봇 API 표준, 비동기 지원, 문서 우수

#### 인프라 및 호스팅

**MVP:**
- AWS EC2 t3.small (또는 로컬 서버)
- PostgreSQL: 로컬 또는 AWS RDS
- Redis: 로컬 또는 AWS ElastiCache
- 모든 서비스 Docker Compose로 통합

**Docker Compose 구성:**
```yaml
services:
  backend:       # FastAPI
  postgres:      # PostgreSQL
  redis:         # Celery
  milvus:        # 벡터 DB
  etcd:          # Milvus 의존성
  minio:         # Milvus 스토리지
```

**선택 이유:** 단일 EC2에서 모든 서비스 실행 (비용 최소화), docker-compose up으로 즉시 실행

#### 보안

**API 키 관리:** 환경 변수 (.env)

**개인정보:** 텔레그램 user_id만 저장

**선택 이유:** 간단하고 Git 커밋 방지, 최소 수집으로 규제 리스크 감소

#### CI/CD

**MVP:** GitHub Actions (무료), 자동 테스트, 수동 배포

**Phase 2:** 자동 배포 (AWS CodeDeploy)

**선택 이유:** GitHub Actions 무료, MVP는 수동 배포로 충분

#### 모니터링 및 로깅

**MVP:** Python logging (파일 로그), 수동 모니터링

**Phase 2:** AWS CloudWatch, Sentry

**선택 이유:** MVP는 간단한 로깅으로 충분

#### 개발 도구

**IDE:** VSCode 권장
**버전 관리:** Git + GitHub
**의존성:** pip + requirements.txt
**코드 포맷:** Black
**Linter:** Flake8

**선택 이유:** 표준 Python 개발 도구

### Known Technical Risks

PM Checklist 검증 과정에서 식별된 기술 리스크 및 완화 전략입니다.

#### HIGH RISK

**1. Milvus 운영 경험 부족**
- **설명**: 팀이 벡터 DB 운영 경험이 없어 성능 튜닝, 장애 대응에 어려움 예상
- **영향**: 벡터 검색 속도 저하, 데이터 손실 가능성
- **완화 전략**:
  - Epic 1 시작 전 Milvus 공식 문서 학습 (1일)
  - 로컬 환경에서 충분한 테스트 (스토리 1.3 완료 후 1일 버퍼)
  - Milvus 커뮤니티 Slack 채널 가입 및 질문 준비
  - 최소 100건 벡터 삽입/검색 성능 테스트 완료 후 다음 단계 진행

**2. 뉴스 크롤링 안정성**
- **설명**: 언론사 사이트 구조 변경, Anti-scraping 정책으로 크롤러 중단 위험
- **영향**: 데이터 수집 중단 → 예측 품질 저하
- **완화 전략**:
  - 3개 이상 다중 소스 지원 (네이버, 한경, 매경)
  - 백업 뉴스 API 준비 (네이버 뉴스 API)
  - 크롤러 실패율 5% 초과 시 자동 알람
  - 사이트 구조 변경 감지 로직 (예상 필드 누락 시 알람)

#### MEDIUM RISK

**3. LLM API 비용 예측 불확실성**
- **설명**: 실사용 패턴(알림 빈도, 프롬프트 길이)에 따라 비용 급증 가능
- **영향**: $50 예산 초과 → MVP 중단
- **완화 전략**:
  - 일일 비용 모니터링 대시보드 (/metrics 엔드포인트)
  - $10/일 초과 시 알람
  - $40 누적 시 프롬프트 최적화 (토큰 수 20% 감축)
  - $50 도달 시 GPT-4o-mini → GPT-3.5-turbo 다운그레이드

**4. 종목코드 매칭 정확도**
- **설명**: 뉴스에서 기업명 추출 및 종목코드 매칭 오류 가능
- **영향**: 잘못된 주가 데이터 → 예측 오류
- **완화 전략**:
  - 기업명-종목코드 매핑 테이블 수동 검증 (상위 100개)
  - 매칭 신뢰도 점수 도입 (정규표현식 기반)
  - 낮은 신뢰도 매칭은 수동 검증 큐에 추가
  - 매칭 로그 추적 및 주간 검토

### Areas Requiring Architectural Investigation

Architect가 상세 설계 시 조사 및 결정이 필요한 영역입니다.

#### 1. Milvus 성능 튜닝
**배경**: 벡터 검색 성능이 예측 응답 시간에 직접 영향
**조사 필요 항목**:
- 인덱스 타입 선택: IVF_FLAT vs HNSW vs IVF_SQ8
  - IVF_FLAT: 정확도 높음, 속도 보통
  - HNSW: 속도 빠름, 메모리 많이 사용
  - 권장: 초기 IVF_FLAT, 성능 이슈 시 HNSW 전환
- 인덱스 파라미터 최적화:
  - nlist: 클러스터 수 (권장: sqrt(N), N=벡터 수)
  - nprobe: 검색 시 탐색 클러스터 수 (권장: 10-20)
  - metric_type: L2 vs IP(내적) (권장: L2, 거리 기반)
- 성능 목표: 100ms 이내 검색 완료

#### 2. Celery 작업 큐 설계
**배경**: 비동기 작업 실패 시 재시도 및 데드레터 처리 필요
**조사 필요 항목**:
- 작업 우선순위 큐 설계:
  - High: 새 뉴스 예측 작업
  - Normal: 일괄 임베딩 작업
  - Low: 통계 집계 작업
- 재시도 정책:
  - 최대 재시도 횟수: 3회
  - Backoff 전략: exponential (1분, 5분, 15분)
  - 재시도 불가 에러: API 키 오류, 포맷 오류
- Dead Letter Queue:
  - 3회 실패 작업은 DLQ로 이동
  - 수동 검토 및 재처리 프로세스

#### 3. 텔레그램 Rate Limiting 처리
**배경**: 동시 다발적 알림 시 텔레그램 API rate limit 초과 가능
**조사 필요 항목**:
- 텔레그램 API 제한:
  - 개별 사용자: 30 msgs/sec
  - 그룹: 20 msgs/min
- 큐잉 전략:
  - 우선순위: 영향도 점수 순
  - 배치 전송: 10개씩 묶어서 1초 간격
  - 실패 시 재시도 (최대 3회)
- 모니터링:
  - 429 Too Many Requests 에러 추적
  - 발송 지연 시간 메트릭

#### 4. 뉴스 중복 제거 알고리즘
**배경**: 동일 뉴스를 여러 소스에서 수집 시 중복 방지 필요
**조사 필요 항목**:
- 제목 유사도 임계값:
  - Levenshtein 거리 기반: 80% 이상 유사 시 중복
  - 임베딩 기반: 코사인 유사도 0.95 이상
- 중복 판별 기준:
  - 제목 유사도 + 발표 시간 (30분 이내) + 종목코드 일치
- 성능 고려:
  - 최근 24시간 뉴스만 대상
  - 해시 기반 1차 필터링 → 상세 유사도 2차 검증

---

## Epic List

### Epic 1: 데이터 수집 및 저장 인프라

**목표:** 프로젝트 기반 설정을 완료하고, 뉴스/주가 데이터를 자동으로 수집하여 벡터 DB에 저장하는 안정적인 파이프라인을 구축한다. 최소한의 헬스체크 엔드포인트를 제공하여 시스템이 작동 중임을 확인할 수 있다.

### Epic 2: LLM 기반 예측 및 알림 시스템

**목표:** 수집된 데이터를 기반으로 LLM + RAG 예측 엔진을 구현하고, 시간대별 맞춤 전략 메시지를 생성하여 텔레그램으로 자동 알림을 전송하는 완전한 end-to-end 시스템을 완성한다.

---

## Epic 1: 데이터 수집 및 저장 인프라

### Epic 목표

프로젝트의 기술 기반을 구축하고, 뉴스와 주가 데이터를 자동으로 수집하여 AI 분석에 필요한 지식 베이스를 구축합니다. 모든 인프라(PostgreSQL, Milvus, Redis)가 Docker Compose로 관리되며, 10분마다 뉴스를 수집하고 1분마다 주가 데이터를 수집하는 안정적인 파이프라인이 작동합니다. Epic 완료 시, 헬스체크 API를 통해 데이터 수집 상태를 확인할 수 있으며, 최소 500건 이상의 과거 뉴스-주가 패턴이 벡터 DB에 저장되어 있습니다.

---

### Story 1.1: 프로젝트 초기 설정 및 인프라 구성

**As a** 개발자,
**I want** Git 레포지토리, Docker Compose, 환경 변수 설정을 완료하고,
**so that** 로컬과 프로덕션 환경에서 동일하게 개발할 수 있다.

#### Acceptance Criteria

1. Git 레포지토리가 생성되고 `.gitignore`에 `.env`, `milvus_data/`, `__pycache__/` 등이 포함되어 있다.
2. `docker-compose.yml`이 PostgreSQL, Milvus(etcd, minio 포함), Redis를 정의한다.
3. `.env.example` 파일이 필요한 환경 변수 템플릿을 제공한다 (OPENAI_API_KEY, TELEGRAM_BOT_TOKEN, DB_HOST 등).
4. `docker-compose up`으로 모든 서비스가 정상 실행된다.
5. `requirements.txt`가 주요 의존성을 포함한다 (fastapi, pymilvus, psycopg2, openai, python-telegram-bot, celery, redis, apscheduler 등).
6. 프로젝트 디렉토리 구조가 `backend/`, `data/`, `scripts/`, `tests/`, `docs/`로 구성된다.
7. README.md에 로컬 환경 설정 방법이 문서화되어 있다.

---

### Story 1.2: PostgreSQL 데이터베이스 스키마 설계 및 구축

**As a** 시스템,
**I want** 뉴스, 주가, 뉴스-주가 매칭 데이터를 저장할 PostgreSQL 스키마를 구축하고,
**so that** 수집된 데이터를 효율적으로 저장하고 조회할 수 있다.

#### Acceptance Criteria

1. `news` 테이블이 생성되며, 컬럼은 id(PK), title, content, published_at, source, stock_code, created_at을 포함한다.
2. `stock_prices` 테이블이 생성되며, 컬럼은 id(PK), stock_code, date, open, high, low, close, volume을 포함한다.
3. `news_stock_match` 테이블이 생성되며, 컬럼은 id(PK), news_id(FK), stock_code, price_change_1d, price_change_3d, price_change_5d, calculated_at을 포함한다.
4. `telegram_users` 테이블이 생성되며, 컬럼은 id(PK), user_id, subscribed_at, is_active를 포함한다.
5. 인덱스가 news(stock_code, published_at), stock_prices(stock_code, date)에 생성된다.
6. 데이터베이스 마이그레이션 스크립트(`scripts/init_db.py`)가 제공된다.
7. 로컬 테스트: 샘플 데이터 삽입 및 조회가 성공한다.

---

### Story 1.3: Milvus 벡터 DB 설정 및 컬렉션 생성

**As a** 시스템,
**I want** Milvus 벡터 DB를 Docker로 실행하고 뉴스 임베딩 컬렉션을 생성하여,
**so that** 과거 유사 뉴스를 빠르게 검색할 수 있다.

#### Acceptance Criteria

1. Milvus가 Docker Compose의 일부로 포트 19530에서 실행된다.
2. `news_embeddings` 컬렉션이 생성되며, 필드는 id(INT64, PK, auto_id), news_id(INT64), embedding(FLOAT_VECTOR, dim=768), stock_code(VARCHAR), price_change_1d(FLOAT), price_change_3d(FLOAT), price_change_5d(FLOAT)를 포함한다.
3. 인덱스가 embedding 필드에 생성된다 (metric_type: L2, index_type: IVF_FLAT).
4. Python 스크립트(`scripts/init_milvus.py`)로 컬렉션 생성 및 인덱스 구축이 자동화된다.
5. 로컬 테스트: 샘플 벡터 삽입 및 유사도 검색(TOP 5)이 성공한다.
6. Milvus 연결 헬퍼 함수(`backend/db/milvus_client.py`)가 제공된다.

---

### Story 1.4: 뉴스 크롤러 구현 (10분 주기)

**As a** 시스템,
**I want** 10분마다 한국 주요 언론사의 증권 뉴스를 자동으로 수집하여 PostgreSQL에 저장하고,
**so that** 최신 뉴스 데이터가 지속적으로 축적된다.

#### Acceptance Criteria

1. 뉴스 크롤러(`backend/crawlers/news_crawler.py`)가 네이버 뉴스, 한국경제, 매일경제를 크롤링한다.
2. APScheduler로 10분마다 크롤러가 자동 실행된다.
3. 각 뉴스에서 제목, 본문, 발표 시간, 종목코드(기업명 매칭)를 추출하여 `news` 테이블에 저장한다.
4. 중복 뉴스는 제목 유사도로 필터링하여 저장하지 않는다.
5. 크롤링 실패 시 에러 로그를 기록하고, 다음 주기에 재시도한다.
6. 크롤링 성공률을 추적하는 로그가 생성된다.
7. 종목코드 매핑 테이블(`stock_codes.json`)이 제공되어 기업명 → 종목코드 변환이 가능하다.

---

### Story 1.5: 주가 데이터 수집기 구현 (1분 주기)

**As a** 시스템,
**I want** 1분마다 코스피/코스닥 주가 데이터를 수집하여 PostgreSQL에 저장하고,
**so that** 실시간에 가까운 주가 정보를 확보한다.

#### Acceptance Criteria

1. 주가 수집기(`backend/crawlers/stock_crawler.py`)가 FinanceDataReader를 사용하여 주가 데이터를 수집한다.
2. APScheduler로 1분마다 수집기가 자동 실행된다 (장중 9:00~15:30).
3. 대형주(시가총액 상위 50개)를 우선 수집하며, 종목 리스트는 설정 파일로 관리된다.
4. 시가/고가/저가/종가/거래량 데이터를 `stock_prices` 테이블에 저장한다.
5. 장 시작 전(9:00 이전)과 장 마감 후(15:30 이후)는 수집을 중단한다.
6. 수집 실패 시 에러 로그를 기록하고, 다음 주기에 재시도한다.
7. 로컬 테스트: 특정 종목의 1분봉 데이터가 정상 저장된다.

---

### Story 1.6: 뉴스-주가 매칭 및 변동률 계산

**As a** 시스템,
**I want** 뉴스 발표 후 1일/3일/5일 주가 변동률을 자동 계산하여 저장하고,
**so that** LLM이 과거 패턴을 참조할 수 있다.

#### Acceptance Criteria

1. 매칭 스크립트(`backend/crawlers/news_stock_matcher.py`)가 매일 장 마감 후(15:40) 실행된다.
2. 각 뉴스에 대해 발표 시점 주가(T0)와 T+1일, T+3일, T+5일 종가를 조회한다.
3. 변동률 = ((T+N일 종가 - T0 주가) / T0 주가) * 100으로 계산한다.
4. 계산된 변동률을 `news_stock_match` 테이블에 저장한다.
5. 주말/공휴일은 다음 영업일 종가를 사용한다.
6. 매칭 정확도가 90% 이상이다.
7. 로컬 테스트: 특정 뉴스의 변동률이 정확히 계산된다.

---

### Story 1.7: 뉴스 임베딩 및 Milvus 저장

**As a** 시스템,
**I want** 수집된 뉴스를 OpenAI Embedding API로 임베딩하여 Milvus에 저장하고,
**so that** 유사 뉴스 검색이 가능하다.

#### Acceptance Criteria

1. 임베딩 스크립트(`backend/llm/embedder.py`)가 매일 장 마감 후(16:00) 실행된다.
2. 아직 임베딩되지 않은 뉴스를 `news` 테이블에서 조회한다.
3. OpenAI text-embedding-3-small API로 뉴스를 768차원 벡터로 변환한다.
4. 벡터와 메타데이터를 Milvus `news_embeddings` 컬렉션에 저장한다.
5. 임베딩 비용이 1건당 $0.0001 이하로 유지된다.
6. 임베딩 실패 시 에러 로그를 기록하고, 다음 실행 시 재시도한다.
7. 로컬 테스트: 100건 뉴스 임베딩 및 Milvus 저장이 성공한다.

---

### Story 1.8: 초기 데이터 수집 (과거 3개월)

**As a** 개발자,
**I want** 과거 3개월 뉴스 및 주가 데이터를 일괄 수집하는 스크립트를 실행하여,
**so that** MVP 시작 시점에 충분한 학습 데이터(500~1,000건)가 확보된다.

#### Acceptance Criteria

1. 일괄 수집 스크립트(`scripts/initial_data_collection.py`)가 제공된다.
2. 과거 3개월의 증권 뉴스를 크롤링하여 PostgreSQL에 저장한다.
3. 동일 기간의 주가 데이터를 수집하여 PostgreSQL에 저장한다.
4. 뉴스-주가 매칭 및 변동률을 계산하여 저장한다.
5. 모든 뉴스를 임베딩하여 Milvus에 저장한다.
6. 최소 500건 이상의 뉴스가 Milvus에 저장된다.
7. 스크립트 실행 시간은 2시간 이내로 완료된다.
8. 로컬 테스트: 스크립트 완료 후 데이터 건수 확인이 성공한다.

---

### Story 1.9: 헬스체크 API 구현

**As a** 개발자/운영자,
**I want** 시스템 상태를 확인할 수 있는 헬스체크 API를 제공받아,
**so that** 데이터 수집 파이프라인이 정상 작동 중인지 모니터링할 수 있다.

#### Acceptance Criteria

1. FastAPI 헬스체크 엔드포인트 `GET /health`가 구현된다.
2. 응답은 다음 정보를 JSON으로 반환한다:
   - `status`: "healthy" 또는 "unhealthy"
   - `postgres`: 연결 상태 (true/false)
   - `milvus`: 연결 상태 (true/false)
   - `redis`: 연결 상태 (true/false)
   - `news_count`: PostgreSQL 뉴스 건수
   - `vector_count`: Milvus 벡터 건수
   - `last_news_collected`: 마지막 뉴스 수집 시간
3. 모든 서비스 정상이면 HTTP 200, 문제 있으면 HTTP 503 반환.
4. FastAPI 서버가 포트 8000에서 실행되며, `http://localhost:8000/health`로 접근 가능하다.
5. 로컬 테스트: 헬스체크 API 호출 시 정상 응답 반환.

---

## Epic 2: LLM 기반 예측 및 알림 시스템

### Epic 목표

Epic 1에서 구축한 데이터 인프라를 기반으로 실제 투자자에게 가치를 제공하는 예측 및 알림 시스템을 완성합니다. 새로운 뉴스가 발생하면 Milvus에서 유사한 과거 뉴스를 검색하고, LLM이 이를 종합 분석하여 상승/하락 확률과 예측 근거를 생성합니다. 시간대를 자동 판별하여 맞춤 전략 메시지를 작성하고, 영향도 8.0 이상인 경우 텔레그램으로 즉시 알림을 전송합니다. Epic 완료 시, 뉴스 발생부터 텔레그램 알림까지 5분 이내 end-to-end 파이프라인이 완전히 작동합니다.

---

### Story 2.1: 벡터 유사도 검색 구현

**As a** 시스템,
**I want** 새로운 뉴스의 임베딩을 생성하고 Milvus에서 유사한 과거 뉴스 TOP 5를 검색하여,
**so that** LLM이 과거 패턴을 참조할 수 있다.

#### Acceptance Criteria

1. 유사도 검색 함수(`backend/llm/similarity_search.py`)가 구현된다.
2. 입력: 뉴스 텍스트, 출력: 유사 뉴스 TOP 5 (news_id, 유사도, 제목, 종목코드, 변동률)
3. OpenAI text-embedding-3-small로 입력 뉴스를 임베딩한다.
4. Milvus에서 L2 거리 기반 유사도 검색을 수행한다.
5. 검색 결과에서 PostgreSQL과 조인하여 상세 정보를 가져온다.
6. 검색 시간이 100ms 이내로 완료된다.
7. 로컬 테스트: 유사한 뉴스들이 정확히 반환된다.

---

### Story 2.2: LLM 프롬프트 설계 및 RAG 분석 로직

**As a** 시스템,
**I want** 현재 뉴스, 유사 과거 뉴스, 현재 주가를 LLM에 제공하여 종합 분석 결과를 받아,
**so that** 상승/하락 확률 및 예측 근거를 생성할 수 있다.

#### Acceptance Criteria

1. LLM 분석 함수(`backend/llm/predictor.py`)가 구현된다.
2. 입력: 현재 뉴스, 유사 뉴스 TOP 5, 현재 주가 정보
3. LLM 프롬프트가 체계적으로 구성된다 (역할, 현재 뉴스, 과거 유사 뉴스, 질문)
4. OpenAI GPT-4o-mini API를 호출하여 JSON 형식 응답을 받는다.
5. 응답 파싱 후 예측 결과 객체로 반환한다.
6. LLM 응답 시간이 2~5초 이내이다.
7. API 비용이 1건당 $0.02 이하로 유지된다.
8. 로컬 테스트: 실제 뉴스로 유효한 JSON 응답이 반환된다.

---

### Story 2.3: 시간대 판별 로직 구현

**As a** 시스템,
**I want** 뉴스 발표 시간을 기준으로 프리마켓/장중/장후를 자동 판별하여,
**so that** 시간대별 맞춤 전략을 제공할 수 있다.

#### Acceptance Criteria

1. 시간대 판별 함수(`backend/llm/time_classifier.py`)가 구현된다.
2. 입력: 뉴스 발표 시간, 출력: "PRE_MARKET" | "INTRADAY" | "POST_MARKET"
3. 판별 기준:
   - PRE_MARKET: 00:00 ~ 08:59
   - INTRADAY: 09:00 ~ 15:29
   - POST_MARKET: 15:30 ~ 23:59
4. 주말/공휴일은 POST_MARKET으로 처리한다.
5. 한국 시간대(KST) 기준으로 판별한다.
6. 로컬 테스트: 다양한 시간대 입력 시 올바른 분류가 반환된다.

---

### Story 2.4: 시간대별 메시지 생성 템플릿

**As a** 시스템,
**I want** 시간대에 따라 다른 구조의 텔레그램 메시지를 생성하여,
**so that** 사용자에게 상황별 최적의 투자 전략을 제공할 수 있다.

#### Acceptance Criteria

1. 메시지 생성 함수(`backend/telegram/message_builder.py`)가 구현된다.
2. 입력: 예측 결과, 시간대, 뉴스 정보, 주가 정보
3. 프리마켓 템플릿이 예상 시가, 동시호가 전략, 과거 패턴, 전략 옵션을 포함한다.
4. 장중 템플릿이 현재가, 뉴스 반영도(%), 추가 상승 여력, 진입 타이밍을 포함한다.
5. 장후 템플릿이 다음날 예상 시가, 준비 사항, 목표가/손절가를 포함한다.
6. 모든 메시지는 텔레그램 마크다운 포맷을 사용한다.
7. 이모지 사용 규칙을 준수한다.
8. 메시지 길이가 500~800자 범위로 제한된다.
9. 메시지 끝에 면책 조항이 포함된다.
10. 로컬 테스트: 각 시간대별 샘플 데이터로 올바른 포맷이 반환된다.

---

### Story 2.5: 영향도 필터링 및 알림 트리거 로직

**As a** 시스템,
**I want** LLM 예측 결과 중 영향도 8.0 이상인 뉴스만 알림 대상으로 선정하여,
**so that** 중요한 뉴스만 사용자에게 전달하고 알림 피로도를 방지할 수 있다.

#### Acceptance Criteria

1. 알림 트리거 함수(`backend/telegram/notification_filter.py`)가 구현된다.
2. 입력: LLM 예측 결과, 출력: True(알림 전송) | False(전송 안 함)
3. 필터링 기준:
   - 영향도 점수 ≥ 8.0
   - 상승 또는 하락 확률 ≥ 70%
4. 동일 종목에 대해 1시간 내 중복 알림을 방지한다 (Redis 캐시).
5. 필터링 로그를 기록한다.
6. 로컬 테스트: 영향도 7.5는 필터링, 8.5는 통과한다.

---

### Story 2.6: 텔레그램 봇 기본 구현

**As a** 사용자,
**I want** 텔레그램에서 봇을 추가하고 `/start` 명령으로 구독을 시작하여,
**so that** 중요 뉴스 알림을 받을 수 있다.

#### Acceptance Criteria

1. 텔레그램 봇(`backend/telegram/bot.py`)이 python-telegram-bot으로 구현된다.
2. 봇 토큰은 환경 변수에서 로드한다.
3. `/start` 명령 처리: user_id를 DB에 저장하고 환영 메시지 전송
4. `/stop` 명령 처리: is_active를 false로 업데이트
5. `/help` 명령 처리: 사용 가능한 명령어 안내
6. 봇이 FastAPI와 별도 프로세스로 실행된다.
7. 로컬 테스트: 텔레그램에서 `/start` 실행 → 환영 메시지 수신

---

### Story 2.7: 비동기 알림 파이프라인 구현

**As a** 시스템,
**I want** 새 뉴스 감지 시 예측 분석부터 텔레그램 전송까지 비동기로 처리하여,
**so that** 5분 이내 알림을 보낼 수 있다.

#### Acceptance Criteria

1. 뉴스 감지 트리거가 10분마다 실행되며 새 뉴스를 확인한다.
2. 새 뉴스 발견 시 Celery 태스크를 큐에 추가한다.
3. Celery 워커가 순차 실행한다:
   - 벡터 유사도 검색
   - LLM 예측 분석
   - 시간대 판별
   - 메시지 생성
   - 영향도 필터링
   - 텔레그램 전송
4. 전체 파이프라인이 평균 3분 이내 완료된다.
5. 각 단계의 실행 시간을 로그로 기록한다.
6. 에러 발생 시 Celery 재시도 정책이 작동한다 (최대 3회).
7. 로컬 테스트: 새 뉴스 삽입 → 5분 이내 텔레그램 알림 수신

---

### Story 2.8: End-to-End 통합 테스트 및 모니터링

**As a** 개발자/운영자,
**I want** 전체 시스템의 end-to-end 통합 테스트를 수행하고 모니터링 대시보드를 확인하여,
**so that** MVP가 요구사항을 충족하는지 검증할 수 있다.

#### Acceptance Criteria

1. E2E 테스트 스크립트(`tests/e2e_test.py`)가 제공된다.
2. 테스트 시나리오: 뉴스 크롤링 → 주가 수집 → 매칭 → 임베딩 → 예측 → 알림
3. 각 단계의 성공/실패를 로그로 기록한다.
4. 헬스체크 API를 확장하여 last_prediction, telegram_notifications_sent_24h, average_prediction_time 추가
5. 모니터링 엔드포인트 `GET /metrics` 구현
6. 2주 MVP 기간 동안:
   - 데이터 수집 성공률 ≥ 95%
   - 예측 → 알림 시간 평균 ≤ 5분
   - 텔레그램 전송 성공률 ≥ 95%
7. 테스트 사용자 20명 이상 `/start` 구독 완료
8. 실제 중요 뉴스 10건 이상에 대해 알림이 정상 전송됨
9. 사용자 피드백 수집 방법이 문서화됨

---

## Checklist Results Report

**검증 일자:** 2025-10-27
**검증자:** John (Product Manager)
**전체 PRD 완성도:** 87% (PASS)
**아키텍처 준비도:** Nearly Ready ⚠️

### Executive Summary

PM Checklist 검증 결과, PRD는 **87% 완성도**로 아키텍처 단계로 진행 가능합니다. 모든 필수 섹션이 우수한 품질로 작성되었으며, Epic/Story 구조가 명확합니다.

**주요 개선 사항 (v1.1에 반영 완료):**
- ✅ NFR18-21 추가: 백업/복구 및 데이터 보관 정책
- ✅ Known Technical Risks 섹션 추가: 4개 식별된 리스크 및 완화 전략
- ✅ Areas Requiring Investigation 섹션 추가: Architect가 조사할 4개 영역

**종합 평가:** ⭐⭐⭐⭐ (Very Good - 4.3/5.0)

| 평가 항목 | 점수 | 상태 |
|----------|------|------|
| 문제 정의 | ⭐⭐⭐⭐⭐ | Excellent |
| 요구사항 명확성 | ⭐⭐⭐⭐⭐ | Excellent |
| Epic/Story 구조 | ⭐⭐⭐⭐⭐ | Excellent |
| 기술 가이던스 | ⭐⭐⭐⭐ | Very Good |
| 운영 준비도 | ⭐⭐⭐ | Good |
| 문서 품질 | ⭐⭐⭐⭐ | Very Good |

### Category Status

| Category | Status | Completion |
|----------|--------|------------|
| 1. Problem Definition & Context | PASS | 90% |
| 2. MVP Scope Definition | PASS | 95% |
| 3. User Experience Requirements | PASS | 85% |
| 4. Functional Requirements | PASS | 100% |
| 5. Non-Functional Requirements | PASS | 92% |
| 6. Epic & Story Structure | PASS | 100% |
| 7. Technical Guidance | PASS | 90% |
| 8. Cross-Functional Requirements | PARTIAL | 80% |
| 9. Clarity & Communication | PARTIAL | 75% |

### Final Decision

✅ **READY FOR ARCHITECT**

PRD v1.1은 아키텍처 단계로 진행하기에 충분한 품질입니다. PM Checklist에서 식별된 HIGH 우선순위 이슈가 모두 해결되었습니다.

**다음 단계:**
1. Architect에게 PRD 전달
2. 시스템 아키텍처 다이어그램 생성 요청
3. Technical Investigation 영역 상세 분석 요청

**상세 검증 보고서:** [docs/pm-checklist-results.md](./pm-checklist-results.md)

---

## Next Steps

### UX Expert Prompt

UX Expert님께,

Craveny 프로젝트의 PRD가 완성되었습니다. 본 PRD를 검토하시고, 텔레그램 메시지 UX를 구체화하는 디자인 작업을 시작해 주세요. "User Interface Design Goals" 섹션에 텔레그램 메시지 포맷 가이드라인이 정의되어 있으니 참고 부탁드립니다.

**주요 작업:**
- 프리마켓/장중/장후 메시지 템플릿 최종 디자인
- 정보 계층 구조 검증
- 사용자 테스트 계획 수립

### Architect Prompt

Architect님께,

Craveny 프로젝트의 PRD가 완성되었습니다. 본 PRD의 "Technical Assumptions" 섹션과 Epic 1, Epic 2의 스토리들을 검토하시고, 상세 아키텍처 문서 작성을 시작해 주세요.

**주요 작업:**
- Docker Compose 전체 구성 상세화
- FastAPI 서버 구조 설계
- Milvus 연동 아키텍처
- Celery 비동기 작업 플로우
- 에러 핸들링 및 모니터링 전략

create architecture mode를 사용하여 이 PRD를 입력으로 아키텍처 문서를 생성해 주세요.

---

*PRD v1.1 검증 완료 - 2025-10-27*
*BMAD-METHOD™ 프레임워크 사용*
*기반 문서: docs/brief.md, docs/brainstorming-session-results.md*
*PM Checklist 검증: docs/pm-checklist-results.md*
