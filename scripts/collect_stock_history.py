#!/usr/bin/env python
"""
ì£¼ê°€ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

target_stocks.jsonì˜ ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ ê³¼ê±° 3ê°œì›” ì£¼ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
ì‹ ê·œ ì¢…ëª©ì´ ì¶”ê°€ë˜ì—ˆì„ ë•Œ ë˜ëŠ” ì´ˆê¸° ì„¤ì • ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import logging
from datetime import datetime, timedelta
from typing import Optional

from backend.crawlers.stock_crawler import get_stock_crawler
from backend.db.session import SessionLocal
from backend.db.models.stock import StockPrice
from sqlalchemy import func

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def get_earliest_date_for_stock(db, stock_code: str) -> Optional[datetime]:
    """
    íŠ¹ì • ì¢…ëª©ì˜ ê°€ì¥ ì˜¤ë˜ëœ ì£¼ê°€ ë°ì´í„° ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
        stock_code: ì¢…ëª© ì½”ë“œ

    Returns:
        ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œ ë˜ëŠ” None (ë°ì´í„° ì—†ìŒ)
    """
    min_date = db.query(func.min(StockPrice.date)).filter(
        StockPrice.stock_code == stock_code
    ).scalar()
    return min_date


def collect_history_for_stock(stock_code: str, stock_name: str, days: int = 90) -> int:
    """
    íŠ¹ì • ì¢…ëª©ì˜ ê³¼ê±° ì£¼ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Args:
        stock_code: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª…
        days: ìˆ˜ì§‘í•  ê³¼ê±° ì¼ìˆ˜ (ê¸°ë³¸ê°’: 90ì¼)

    Returns:
        ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
    """
    logger.info("=" * 70)
    logger.info(f"ğŸ“ˆ {stock_name} ({stock_code}) ê³¼ê±° ì£¼ê°€ ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 70)

    db = SessionLocal()
    stock_crawler = get_stock_crawler()

    try:
        # ê¸°ì¡´ ë°ì´í„° í™•ì¸
        earliest_date = get_earliest_date_for_stock(db, stock_code)
        existing_count = db.query(StockPrice).filter(
            StockPrice.stock_code == stock_code
        ).count()

        if earliest_date:
            logger.info(f"ê¸°ì¡´ ë°ì´í„°: {existing_count}ê±´ (ê°€ì¥ ì˜¤ë˜ëœ: {earliest_date.date()})")
        else:
            logger.info(f"ê¸°ì¡´ ë°ì´í„°: ì—†ìŒ")

        # ìˆ˜ì§‘ ê¸°ê°„ ê³„ì‚°
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê·¸ ì´ì „ë¶€í„°ë§Œ ìˆ˜ì§‘
        if earliest_date:
            # ê¸°ì¡´ ë°ì´í„°ê°€ ì¶©ë¶„íˆ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ìŠ¤í‚µ
            target_start_date = end_date - timedelta(days=days)
            if earliest_date.date() <= target_start_date.date():
                logger.info(f"âœ… ì¶©ë¶„í•œ ê³¼ê±° ë°ì´í„° ì¡´ì¬ ({days}ì¼ ì´ìƒ). ìˆ˜ì§‘ ìŠ¤í‚µ")
                return 0

            # ê¸°ì¡´ ë°ì´í„° ì´ì „ë¶€í„° ìˆ˜ì§‘
            end_date = earliest_date - timedelta(days=1)
            logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: {start_date.date()} ~ {end_date.date()}")
        else:
            logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days}ì¼ ({start_date.date()} ~ {end_date.date()})")

        # ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘
        df = stock_crawler.fetch_stock_data(stock_code, start_date=start_date)

        if df is None or df.empty:
            logger.warning(f"âš ï¸  {stock_name}: ìˆ˜ì§‘ëœ ì£¼ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0

        logger.info(f"âœ… {len(df)}ê±´ì˜ ì£¼ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤")

        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        saved_count = stock_crawler.save_stock_data(stock_code, df, db)

        logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {saved_count}ê±´")

        # ìµœì¢… í†µê³„
        total_count = db.query(StockPrice).filter(
            StockPrice.stock_code == stock_code
        ).count()
        new_earliest = get_earliest_date_for_stock(db, stock_code)
        new_latest = db.query(func.max(StockPrice.date)).filter(
            StockPrice.stock_code == stock_code
        ).scalar()

        logger.info(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {total_count}ê±´ ({new_earliest.date()} ~ {new_latest.date()})")
        logger.info("")

        return saved_count

    except Exception as e:
        logger.error(f"âŒ {stock_name} ì£¼ê°€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", exc_info=True)
        db.rollback()
        return 0

    finally:
        db.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì£¼ê°€ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    logger.info("")

    # ì£¼ê°€ ìˆ˜ì§‘ê¸°ì—ì„œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    stock_crawler = get_stock_crawler()

    # ëª¨ë“  ìš°ì„ ìˆœìœ„ì˜ ì¢…ëª© ìˆ˜ì§‘
    total_collected = 0
    total_stocks = len(stock_crawler.target_stocks)

    logger.info(f"ëŒ€ìƒ ì¢…ëª©: {total_stocks}ê°œ")
    logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ 90ì¼")
    logger.info("")

    # ê° ì¢…ëª©ë³„ ê³¼ê±° ì£¼ê°€ ìˆ˜ì§‘
    for stock in stock_crawler.target_stocks:
        count = collect_history_for_stock(
            stock_code=stock["code"],
            stock_name=stock["name"],
            days=90  # 3ê°œì›”
        )
        total_collected += count

    # ìµœì¢… ê²°ê³¼
    logger.info("=" * 70)
    logger.info("âœ… ì£¼ê°€ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    logger.info("=" * 70)
    logger.info(f"ì´ ìˆ˜ì§‘ ë ˆì½”ë“œ: {total_collected}ê±´")
    logger.info("")


if __name__ == "__main__":
    main()
