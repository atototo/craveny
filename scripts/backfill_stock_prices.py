"""
11ì›” 1-10ì¼ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
í‰ê°€ ì‹œìŠ¤í…œì„ ìœ„í•œ ê³¼ê±° ì£¼ê°€ ë°ì´í„° ë°±í•„
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from datetime import datetime, date
from backend.db.session import SessionLocal
from backend.crawlers.stock_crawler import StockCrawler
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    logger.info("\n" + "="*80)
    logger.info("ğŸ“ˆ 11ì›” 1-10ì¼ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘")
    logger.info("="*80)

    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = StockCrawler(use_db=True)
    stock_codes = crawler.get_stock_codes()

    logger.info(f"ğŸ“Š ìˆ˜ì§‘ ëŒ€ìƒ: {len(stock_codes)}ê°œ ì¢…ëª©")

    # 11ì›” 1ì¼ë¶€í„° ìˆ˜ì§‘
    start_date = datetime(2025, 11, 1)

    db = SessionLocal()
    try:
        total_saved = 0
        success_count = 0

        for stock_code in stock_codes:
            logger.info(f"\nğŸ”„ {stock_code} ìˆ˜ì§‘ ì¤‘...")

            # ë°ì´í„° ìˆ˜ì§‘
            df = crawler.fetch_stock_data(stock_code, start_date=start_date)

            if df is not None and not df.empty:
                # ì €ì¥
                saved = crawler.save_stock_data(stock_code, df, db)
                total_saved += saved

                if saved > 0:
                    success_count += 1
                    logger.info(f"âœ… {stock_code}: {saved}ê±´ ì €ì¥")
                else:
                    logger.warning(f"âš ï¸  {stock_code}: ì €ì¥ ì‹¤íŒ¨")
            else:
                logger.warning(f"âš ï¸  {stock_code}: ë°ì´í„° ì—†ìŒ")

        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        logger.info(f"  - ì„±ê³µ: {success_count}/{len(stock_codes)}ê°œ ì¢…ëª©")
        logger.info(f"  - ì´ ì €ì¥: {total_saved}ê±´")
        logger.info("="*80)

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        db.rollback()
    finally:
        db.close()


if __name__ == "__main__":
    main()
