"""
ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸

ëŒ€ìƒ ì¢…ëª©ì˜ ìµœê·¼ 30ì¼ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import logging
from datetime import datetime, timedelta

from backend.crawlers.naver_search_crawler import NaverNewsSearchCrawler
from backend.db.session import SessionLocal
from backend.db.models.news import NewsArticle

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ëŒ€ìƒ ì¢…ëª© ì •ì˜
TARGET_STOCKS = [
    {"code": "005930", "name": "ì‚¼ì„±ì „ì"},
    {"code": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
    {"code": "005380", "name": "í˜„ëŒ€ì°¨"},
    {"code": "035420", "name": "ë„¤ì´ë²„"},
    {"code": "003670", "name": "í¬ìŠ¤ì½”í“¨ì²˜ì— "},
]

# ìˆ˜ì§‘ ê¸°ê°„
DAYS_TO_COLLECT = 30


def collect_news_for_stock(stock_code: str, stock_name: str, days: int = 30) -> int:
    """
    íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Args:
        stock_code: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª…
        days: ìˆ˜ì§‘í•  ê¸°ê°„ (ì¼)

    Returns:
        ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê°œìˆ˜
    """
    logger.info("=" * 60)
    logger.info(f"ğŸ“° {stock_name} ({stock_code}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 60)

    db = SessionLocal()
    crawler = NaverNewsSearchCrawler()

    try:
        # ê¸°ê°„ ê³„ì‚°
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: {start_date.date()} ~ {end_date.date()}")

        # ë‰´ìŠ¤ ìˆ˜ì§‘
        news_items = crawler.search_news(
            query=stock_name, start_date=start_date, end_date=end_date, limit=100
        )

        if not news_items:
            logger.warning(f"âš ï¸  {stock_name}: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0

        logger.info(f"âœ… {len(news_items)}ê±´ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤")

        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        saved_count = 0
        duplicate_count = 0

        for item in news_items:
            # ì¤‘ë³µ í™•ì¸ (ì œëª©ìœ¼ë¡œ ì²´í¬ - url í•„ë“œê°€ ì—†ìŒ)
            existing = (
                db.query(NewsArticle).filter(NewsArticle.title == item.title).first()
            )

            if existing:
                duplicate_count += 1
                continue

            # ìƒˆ ë‰´ìŠ¤ ì €ì¥ (url í•„ë“œëŠ” í˜„ì¬ ëª¨ë¸ì— ì—†ìŒ)
            news = NewsArticle(
                title=item.title,
                content=item.content,
                source=item.source,
                published_at=item.published_at,
                stock_code=stock_code,
            )

            db.add(news)
            saved_count += 1

        db.commit()

        logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {saved_count}ê±´ (ì¤‘ë³µ ì œì™¸: {duplicate_count}ê±´)")
        logger.info("")

        return saved_count

    except Exception as e:
        logger.error(f"âŒ {stock_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}", exc_info=True)
        db.rollback()
        return 0

    finally:
        db.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    logger.info(f"ëŒ€ìƒ ì¢…ëª©: {len(TARGET_STOCKS)}ê°œ")
    logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {DAYS_TO_COLLECT}ì¼")
    logger.info("")

    total_collected = 0

    # ê° ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
    for stock in TARGET_STOCKS:
        count = collect_news_for_stock(
            stock_code=stock["code"],
            stock_name=stock["name"],
            days=DAYS_TO_COLLECT,
        )
        total_collected += count

    # ìµœì¢… ê²°ê³¼
    logger.info("=" * 60)
    logger.info("âœ… ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    logger.info("=" * 60)
    logger.info(f"ì´ ìˆ˜ì§‘ ë‰´ìŠ¤: {total_collected}ê±´")
    logger.info("")

    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    logger.info("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    logger.info("   1. ë‰´ìŠ¤ ì„ë² ë”©: uv run python -m backend.llm.embedder")
    logger.info("   2. ì£¼ê°€ ë§¤ì¹­: uv run python -m backend.crawlers.news_stock_matcher")
    logger.info("")


if __name__ == "__main__":
    main()
