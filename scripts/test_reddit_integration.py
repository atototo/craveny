"""
Reddit í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

Reddit í¬ë¡¤ë§ â†’ DB ì €ì¥ â†’ ì˜ˆì¸¡ ìƒì„±ê¹Œì§€ ì „ì²´ í”Œë¡œìš°ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""
import logging
from datetime import datetime

from backend.db.session import SessionLocal
from backend.crawlers.reddit_crawler import RedditCrawler
from backend.crawlers.news_saver import NewsSaver


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_reddit_integration():
    """Reddit í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    logger.info("=" * 60)
    logger.info("Reddit í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)

    # 1. Reddit í¬ë¡¤ë§
    logger.info("\n1ï¸âƒ£ Reddit í¬ë¡¤ë§ ì‹œì‘...")
    crawler = RedditCrawler(
        subreddits=['investing'],  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 1ê°œë§Œ
        min_upvotes=10,
        min_comments=2,
        lookback_hours=24,
    )

    news_list = crawler.fetch_news(limit=20)
    logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(news_list)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")

    if not news_list:
        logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì¢…ë£Œ.")
        return

    # 2. DB ì €ì¥
    logger.info("\n2ï¸âƒ£ DB ì €ì¥ ì‹œì‘...")
    db = SessionLocal()

    try:
        saver = NewsSaver(db, auto_predict=True)  # ìë™ ì˜ˆì¸¡ í™œì„±í™”

        saved_count, skipped_count = saver.save_news_batch(news_list)

        logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: ì €ì¥ {saved_count}ê±´, ì¤‘ë³µ ìŠ¤í‚µ {skipped_count}ê±´")

        # 3. ê²°ê³¼ í™•ì¸
        logger.info("\n3ï¸âƒ£ ê²°ê³¼ í™•ì¸...")

        from sqlalchemy import text

        # Reddit ê²Œì‹œê¸€ ìˆ˜ í™•ì¸
        result = db.execute(text("""
            SELECT COUNT(*) as count
            FROM news_articles
            WHERE content_type = 'reddit'
        """))
        reddit_count = result.scalar()
        logger.info(f"ğŸ“Š DBì— ì €ì¥ëœ Reddit ê²Œì‹œê¸€: {reddit_count}ê±´")

        # ìµœê·¼ Reddit ê²Œì‹œê¸€ í™•ì¸
        result = db.execute(text("""
            SELECT
                id,
                title,
                author,
                subreddit,
                upvotes,
                num_comments,
                stock_code,
                created_at
            FROM news_articles
            WHERE content_type = 'reddit'
            ORDER BY created_at DESC
            LIMIT 5
        """))

        logger.info("\nìµœê·¼ Reddit ê²Œì‹œê¸€ 5ê°œ:")
        for row in result:
            logger.info(
                f"  - ID={row.id}, ì œëª©='{row.title[:50]}...', "
                f"ì‘ì„±ì={row.author}, subreddit={row.subreddit}, "
                f"upvotes={row.upvotes}, comments={row.num_comments}, "
                f"ì¢…ëª©={row.stock_code or 'N/A'}"
            )

        # ì˜ˆì¸¡ ìƒì„± í™•ì¸
        result = db.execute(text("""
            SELECT COUNT(*) as count
            FROM predictions p
            JOIN news_articles n ON p.news_id = n.id
            WHERE n.content_type = 'reddit'
        """))
        prediction_count = result.scalar()
        logger.info(f"\nğŸ“ˆ Reddit ê²Œì‹œê¸€ì— ëŒ€í•œ ì˜ˆì¸¡: {prediction_count}ê±´")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… Reddit í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}", exc_info=True)
        db.rollback()
    finally:
        db.close()


if __name__ == "__main__":
    test_reddit_integration()
